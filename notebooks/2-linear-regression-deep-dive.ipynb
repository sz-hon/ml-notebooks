{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression in depth\n",
    "==========================\n",
    "\n",
    "*This is based on the course of [Fraida Fund](https://colab.research.google.com/github/ffund/ml-notebooks/blob/master/notebooks/1-colab-tour.ipynb) for  NYU Tandon School of Engineering*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# for 3d interactive plots\n",
    "from ipywidgets import interact, fixed, widgets\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data generated by a linear function\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a process that generates data as\n",
    "\n",
    "$$y_i = w_0 + w_1 x_{i,1} + \\ldots + w_d x_{i,d} + \\epsilon_i $$\n",
    "\n",
    "where $\\epsilon_i \\sim N(0, \\sigma^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: in this example, we use a “stochastic error” term. This is not to\n",
    "be confused with a residual term which can include systematic,\n",
    "non-random error.\n",
    "\n",
    "-   stochastic error: difference between observed value and “true”\n",
    "    value. These random errors are independent, not systematic, and\n",
    "    cannot be “learned” by any machine learning model.\n",
    "-   residual: difference between observed value and estimated value.\n",
    "    These errors are typical *not* independent, and they can be\n",
    "    systematic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s a function to generate this kind of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linear_regression_data(n=100, d=1, coef=[5], intercept=1, sigma=0):\n",
    "  x = np.random.randn(n,d)\n",
    "  y = (np.dot(x, coef) + intercept).squeeze() + sigma * np.random.randn(n)\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and some default values we’ll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "coef = [5]\n",
    "intercept = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple linear regression\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = generate_linear_regression_data(n=n_samples, d=1, coef=coef, intercept=intercept)\n",
    "x_test,  y_test  = generate_linear_regression_data(n=50, d=1, coef=coef, intercept=intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=x_train.squeeze(), y=y_train, s=50);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we generated `x` as a 2D array with `n_samples` rows and 1 column,\n",
    "but to plot it we need a 1D array. In our “crash course” lecture, we\n",
    "introduced the `squeeze()` function that removes any dimension with size\n",
    "1, so the result here is a 1D array. We could also have used\n",
    "`x_train.reshape(-1,)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the “classical machine learning” part of this course, we’ll use\n",
    "`scikit-learn` implementations of most ML models. These all follow the\n",
    "same standard format, so once you learn how to use one, you know the\n",
    "basic usage of all of them.\n",
    "\n",
    "The basic format is:\n",
    "\n",
    "``` python\n",
    "m = Model()        # create an instance of the model - whatever type it is\n",
    "m.fit(x_tr, y_tr)  # fit the model using the training data. Note: x_tr must be 2D\n",
    "                   # if x_tr is 1D, make it 2D by passing x_tr.reshape((-1,1)) or x_tr[:,None]\n",
    "\n",
    "y_tr_hat = m.predict(x_tr)  # now get model prediction on training data (note: x_tr still must be 2D!)\n",
    "y_ts_hat = m.predict(x_ts)  # also get model prediction on test data (note: x_ts must be 2D!)\n",
    "\n",
    "\n",
    "metrics.mean_squared_error(y_ts, y_ts_hat)  # for regression: get error on the test data\n",
    "metrics.r2_score(y_ts, y_ts_hat)            # or R2 on the test data\n",
    "m.score(x_ts, y_ts)                         # another way to get test data performance (R2 for regression)\n",
    "```\n",
    "\n",
    "Many models have some additional arguments you can set, or parameters\n",
    "you can check after fitting - check the documentation for details. For\n",
    "example, you can review the\n",
    "[LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "model documentation.\n",
    "\n",
    "Here’s how this would apply for a `LinearRegression()` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_simple = LinearRegression().fit(x_train, y_train)\n",
    "print(\"Intercept: \" , reg_simple.intercept_)\n",
    "print(\"Coefficient list: \", reg_simple.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_line = [np.min(x_train), np.max(x_train)]\n",
    "y_line = x_line*reg_simple.coef_ + reg_simple.intercept_\n",
    "sns.scatterplot(x=x_test.squeeze(), y=y_test, s=50, color=sns.color_palette()[3]);\n",
    "sns.scatterplot(x=x_train.squeeze(), y=y_train, s=50);\n",
    "sns.lineplot(x=x_line, y=y_line, color=sns.color_palette()[1]);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: other ways to do the same thing...\n",
    "# first, add a ones column to design matrix\n",
    "x_tilde = np.hstack((np.ones((n_samples, 1)), x_train))\n",
    "\n",
    "# using matrix operations to find w = (X^T X)^{-1} X^T y\n",
    "print( (np.linalg.inv((x_tilde.T @ x_tilde)) @ x_tilde.T @ y_train) )\n",
    "\n",
    "# using solve on normal equations: X^T X w = X^T y\n",
    "# solve only works on matrix that is square and of full-rank\n",
    "# see https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html\n",
    "print( np.linalg.solve(x_tilde.T @ x_tilde, x_tilde.T @ y_train) )\n",
    "\n",
    "# using the lstsq solver \n",
    "# problem may be under-, well-, or over-determined\n",
    "# see https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html\n",
    "print( np.linalg.lstsq(x_tilde,y_train,rcond=0)[0] ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The mean-removed equivalent\n",
    "\n",
    "Quick digression - what if we don’t want to bother with intercept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_mr = x_train - np.mean(x_train)\n",
    "y_train_mr = y_train - np.mean(y_train)\n",
    "sns.scatterplot(x=x_train_mr.squeeze(), y=y_train_mr, s=50);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that now the data is mean removed - zero mean in every dimension.\n",
    "(Removing the mean is also called *centering* the data.)\n",
    "\n",
    "This time, the fitted linear regression has 0 intercept:\n",
    "\n",
    "(We could have specified `fit_intercept=False` as an argument to the\n",
    "model, but we didn’t so that we could see for ourselves that the\n",
    "intercept is zero!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_mr = LinearRegression().fit(x_train_mr, y_train_mr)\n",
    "print(\"Intercept: \" , reg_mr.intercept_)\n",
    "print(\"Coefficient list: \", reg_mr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: when pre-processing data (for example, scaling, or removing\n",
    "the mean), we will always use the training data *only* to get the\n",
    "pre-processing parameters. For example, to get the mean-removed test\n",
    "data we would use\n",
    "\n",
    "``` python\n",
    "x_test_mr = x_test - np.mean(x_train)\n",
    "y_test_mr = y_test - np.mean(y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict some new points\n",
    "\n",
    "OK, now we can predict some new points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = reg_simple.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_line = [np.min(x_test), np.max(x_test)]\n",
    "y_line = x_line*reg_simple.coef_ + reg_simple.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=x_line, y=y_line, color=sns.color_palette()[1]);\n",
    "sns.scatterplot(x=x_test.squeeze(), y=y_test_hat, s=50, color=sns.color_palette()[2]);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model, we will compute the MSE on the test data (*not*\n",
    "the data used to find the parameters).\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - (w_0 + w_1 x_i)) ^2$$\n",
    "\n",
    "Use $\\hat{y}_i = w_0 + w_1 x_i$, then\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i) ^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s the `numpy` way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = reg_simple.intercept_ + np.dot(x_test,reg_simple.coef_)\n",
    "mse_simple = 1.0/(len(y_test)) * np.sum((y_test - y_test_hat)**2)\n",
    "mse_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s the `scikit-learn` way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to do the same thing using sklearn\n",
    "y_test_hat = reg_simple.predict(x_test)\n",
    "metrics.mean_squared_error(y_test, y_test_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize MSE for different coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = np.arange(2, 8, 0.5)\n",
    "\n",
    "x_line_c = np.array([np.min(x_train), np.max(x_train)])\n",
    "y_line_c = coefs.reshape(-1,1)*x_line_c.reshape(1,-1) + reg_simple.intercept_\n",
    "\n",
    "p = sns.scatterplot(x=x_test.squeeze(), y=y_test_hat, s=50);\n",
    "p = plt.xlabel('x')\n",
    "p = plt.ylabel('y')\n",
    "for idx, c in enumerate(coefs):\n",
    "  p = sns.lineplot(x=x_line_c, y=y_line_c[idx], color=sns.color_palette()[1], alpha=0.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat_c = coefs.reshape(-1,1)*x_test.reshape(1,-1) + reg_simple.intercept_\n",
    "mses_c = 1.0/(len(y_test)) * np.sum((y_test - y_test_hat_c)**2, axis=1)\n",
    "\n",
    "sns.lineplot(x=coefs, y=mses_c);\n",
    "sns.scatterplot(x=coefs, y=mses_c, s=50);\n",
    "sns.scatterplot(x=reg_simple.coef_, y=mse_simple, color=sns.color_palette()[1], s=100);\n",
    "p = plt.xlabel('w1');\n",
    "p = plt.ylabel('Test MSE');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance, explained variance, R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick reminder:\n",
    "\n",
    "Mean of $x$ and $y$:\n",
    "\n",
    "$$\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i, \\quad \\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$$\n",
    "\n",
    "Sample variance of $x$ and $y$:\n",
    "\n",
    "$$\\sigma_x^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x}) ^2, \\quad \\sigma_y^2 = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\bar{y}) ^2$$\n",
    "\n",
    "Sample covariance of $x$ and $y$:\n",
    "\n",
    "$$\\sigma_{xy} = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_y = 1.0/len(y_test) * np.sum((y_test - np.mean(y_test))**2) # or use np.var()\n",
    "var_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_y = np.mean(y_test)\n",
    "mean_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance of $y$ is the mean sum of the squares of the distances from\n",
    "each $y_i$ to $\\bar{y}$. These distances are illustrated here:\n",
    "\n",
    "-   the horizontal line shows $\\bar{y}$\n",
    "-   each vertical line is a distance from a $y_i$ to $\\bar{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hlines(y=mean_y, xmin=np.min(x_test), xmax=np.max(x_test));\n",
    "plt.vlines(x_test, ymin=mean_y, ymax=y_test, alpha=0.5, color=sns.color_palette()[3]);\n",
    "sns.scatterplot(x=x_test.squeeze(), y=y_test, color=sns.color_palette()[2], s=50);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s look at a similar kind of plot, but with distances to the\n",
    "regression line instead of the to mean line:\n",
    "\n",
    "-   In the previous plot, each vertical line was a $y_i - \\bar{y}$\n",
    "-   In the following plot, each vertical line is a $y_i - \\hat{y}_i$\n",
    "\n",
    "(where $\\hat{y}_i$ is the prediction of the linear regression for a\n",
    "given sample $i$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_test, y_test_hat);\n",
    "plt.vlines(x_test, ymin=y_test, ymax=y_test_hat, color=sns.color_palette()[3], alpha=0.5);\n",
    "sns.scatterplot(x=x_test.squeeze(), y=y_test, color=sns.color_palette()[2], s=50);\n",
    "x_line = [np.min(x_test), np.max(x_test)]\n",
    "y_line = x_line*reg_simple.coef_ + reg_simple.intercept_\n",
    "sns.lineplot(x=x_line, y=y_line, color=sns.color_palette()[1]);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two plots together show how well the variance of $y$ is\n",
    "“explained” by the linear regression model:\n",
    "\n",
    "-   The total variance of $y$ is shown in the first plot, where each\n",
    "    vertical line is\n",
    "\n",
    "$$y_i - \\bar{y}$$\n",
    "\n",
    "-   The *unexplained* variance of $y$ is shown in the second plot, where\n",
    "    each vertical line is the error of the model,\n",
    "\n",
    "$$y_i - \\hat{y}_i$$\n",
    "\n",
    "In this example, *all* of the variance of $y$ is “explained” by the\n",
    "linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE for this example is 0, R2 is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(intercept_fit = widgets.FloatSlider(min=-8, max=8, step=0.5, value=1),\n",
    "   coef_fit = widgets.FloatSlider(min=-8, max=8, step=0.1, value=5),\n",
    "   show_residual=True)\n",
    "def plot_reg(intercept_fit, coef_fit, show_residual):\n",
    "    x_train, y_train = generate_linear_regression_data(n=20000, d=1, coef=5, intercept=1, sigma=0)\n",
    "    x_test,  y_test =  generate_linear_regression_data(n=10000, d=1, coef=5, intercept=1, sigma=0)\n",
    "    y_test_hat = intercept_fit + coef_fit*x_test\n",
    "    r2_test = metrics.r2_score(y_test, y_test_hat)\n",
    "    mse_test = metrics.mean_squared_error(y_test, y_test_hat)\n",
    "    x_line = np.array([-3, 3])\n",
    "    y_line = intercept_fit + coef_fit*x_line\n",
    "    plt.axhline(y=np.mean(y_train), color=sns.color_palette()[1]);\n",
    "    if show_residual:\n",
    "      plt.vlines(x_test[:100,], ymin=y_test[:100,], ymax=y_test_hat[:100,], alpha=0.5, color=sns.color_palette()[3]);\n",
    "    sns.lineplot(x=x_line, y=y_line, color=sns.color_palette()[2]);\n",
    "    sns.scatterplot(x=x_test[:100,].squeeze(), y=y_test[:100], s=50);\n",
    "    plt.xlabel('x');\n",
    "    plt.ylabel('y');\n",
    "    plt.ylim(-20,20)\n",
    "    plt.xlim(-3,3)\n",
    "    plt.title(\"MSE: %f\\nR2: %f\" % (mse_test, r2_test) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple linear regression with noise\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = generate_linear_regression_data(n=n_samples, d=1, coef=coef, intercept=intercept, sigma=2)\n",
    "x_test,  y_test =  generate_linear_regression_data(n=50, d=1, coef=coef, intercept=intercept, sigma=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=x_train.squeeze(), y=y_train, s=50);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_noisy = LinearRegression().fit(x_train, y_train)\n",
    "print(\"Coefficient list: \", reg_noisy.coef_)\n",
    "print(\"Intercept: \" , reg_noisy.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_line = [np.min(x_train), np.max(x_train)]\n",
    "y_line = x_line*reg_noisy.coef_ + reg_noisy.intercept_\n",
    "\n",
    "sns.scatterplot(x=x_train.squeeze(), y=y_train, s=50);\n",
    "sns.lineplot(x=x_line, y=y_line, color=sns.color_palette()[1]);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict some new points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = reg_noisy.intercept_ + np.dot(x_test,reg_noisy.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_line = [np.min(x_test), np.max(x_test)]\n",
    "y_line = x_line*reg_noisy.coef_ + reg_noisy.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=x_line, y=y_line, color=sns.color_palette()[1]);\n",
    "sns.scatterplot(x=x_test.squeeze(), y=y_test_hat, color=sns.color_palette()[1], s=50);\n",
    "sns.scatterplot(x=x_test.squeeze(), y=y_test, color=sns.color_palette()[2], s=50);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = reg_noisy.intercept_ + np.dot(x_test,reg_noisy.coef_)\n",
    "mse_noisy = 1.0/(len(y_test)) * np.sum((y_test - y_test_hat)**2)\n",
    "mse_noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE is higher than before! (When it was essentially zero.)\n",
    "\n",
    "Does this mean our estimate of $w_0$ and $w_1$ is not optimal?\n",
    "\n",
    "Since we generated the data, we know the “true” coefficient value and we\n",
    "can see how much the MSE would be with the true coefficient values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_perfect_coef = intercept + np.dot(x_test,coef)\n",
    "\n",
    "mse_perfect_coef = 1.0/(len(y_test_perfect_coef)) * np.sum((y_test_perfect_coef - y_test)**2)\n",
    "mse_perfect_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes our linear regression doesn’t select the “true” coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_hat = reg_noisy.intercept_ + np.dot(x_train,reg_noisy.coef_)\n",
    "mse_train_est = 1.0/(len(y_train)) * np.sum((y_train - y_train_hat)**2)\n",
    "mse_train_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_perfect_coef = intercept + np.dot(x_train,coef)\n",
    "mse_train_perfect = 1.0/(len(y_train_perfect_coef)) * np.sum((y_train_perfect_coef - y_train)**2)\n",
    "mse_train_perfect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The “correct” coefficients had slightly higher MSE on the training set\n",
    "than the fitted coefficients. We fit parameters so that they are optimal\n",
    "on the *training* set, then we use the test set to understand how the\n",
    "model will generalize to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that part of the MSE is due to noise in the data, and part is due\n",
    "to error in the parameter estimates.\n",
    "\n",
    "Soon - we will formalize this discussion of different sources of error:\n",
    "\n",
    "-   Error in parameter estimates\n",
    "-   “Noise” - any variation in data that is not a function of the $X$\n",
    "    that we use as input to the model\n",
    "-   Other error - for example, model (hypothesis class) not a good\n",
    "    choice for the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize MSE for different coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = np.arange(4.5, 5.5, 0.1)\n",
    "\n",
    "y_test_hat_c = reg_noisy.intercept_ + np.dot(x_test,coefs.reshape(1,-1))\n",
    "mses_test =  np.mean((y_test.reshape(-1,1) - y_test_hat_c)**2, axis=0)\n",
    "y_train_hat_c = reg_noisy.intercept_ + np.dot(x_train,coefs.reshape(1,-1))\n",
    "mses_train =  np.mean((y_train.reshape(-1,1) - y_train_hat_c)**2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "sns.lineplot(x=coefs, y=mses_train)\n",
    "sns.scatterplot(x=coefs, y=mses_train, s=50);\n",
    "sns.scatterplot(x=reg_noisy.coef_, y=mse_train_est, color=sns.color_palette()[1], s=100);\n",
    "plt.title(\"Training MSE vs. coefficient\");\n",
    "plt.xlabel('w1');\n",
    "plt.ylabel('MSE');\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.lineplot(x=coefs, y=mses_test)\n",
    "sns.scatterplot(x=coefs, y=mses_test, s=50);\n",
    "sns.scatterplot(x=reg_noisy.coef_, y=mse_noisy, color=sns.color_palette()[1], s=100);\n",
    "plt.title(\"Test MSE vs. coefficient\");\n",
    "plt.xlabel('w1');\n",
    "plt.ylabel('MSE');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot on the left (for training MSE), the orange dot (our\n",
    "coefficient estimate) should always have minimum MSE, because we select\n",
    "parameters to minimize MSE on the training set.\n",
    "\n",
    "In the plot on the right (for test MSE), the orange dot might not have\n",
    "the minimum MSE, because the best coefficient on the training set might\n",
    "not be the best coefficient on the test set. This gives us some idea of\n",
    "how our model will generalize to new, unseen data. We may suspect that\n",
    "if the coefficient estimate is not perfect for *this* test data, it\n",
    "might have some error on other new, unseen data, too.\n",
    "\n",
    "If you re-run this notebook many times, you’ll get a new random sample\n",
    "of training and test data each time. Sometimes, the “true” coefficients\n",
    "may have smaller MSE on the test set than the estimated coefficients. On\n",
    "other runs, the estimated coefficients might have smaller MSE on the\n",
    "test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance, explained variance, R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_y = 1.0/len(y_test) * np.sum((y_test - np.mean(y_test))**2)\n",
    "var_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_y = np.mean(y_test)\n",
    "mean_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_line = [np.min(x_test), np.max(x_test)]\n",
    "y_line = x_line*reg_noisy.coef_ + reg_noisy.intercept_\n",
    "plt.hlines(mean_y, xmin=np.min(x_test), xmax=np.max(x_test));\n",
    "plt.vlines(x_test, ymin=mean_y, ymax=y_test, color=sns.color_palette()[4], alpha=0.5);\n",
    "sns.scatterplot(x=x_test.squeeze(), y=y_test, color=sns.color_palette()[2], s=50);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.vlines(x_test, ymin=y_test, ymax=y_test_hat, color=sns.color_palette()[1], alpha=0.5);\n",
    "sns.scatterplot(x=x_test.squeeze(), y=y_test, color=sns.color_palette()[2], s=50);\n",
    "x_line = [np.min(x_test), np.max(x_test)]\n",
    "y_line = x_line*reg_noisy.coef_ + reg_noisy.intercept_\n",
    "sns.lineplot(x=x_line, y=y_line, color=sns.color_palette()[1]);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember:\n",
    "\n",
    "The total variance of $y$ is shown in the first plot, where each\n",
    "vertical line is $y_i - \\bar{y}$\n",
    "\n",
    "The *unexplained* variance of $y$ is shown in the second plot, where\n",
    "each vertical line is the error of the model, $y_i - \\hat{y}_i$\n",
    "\n",
    "In the next plot, we’ll combine them to get some intuition regarding the\n",
    "*fraction of unexplained variance*. The purple part of each vertical bar\n",
    "is the *unexplained* part, while the orange part is *explained* by the\n",
    "linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_line = [np.min(x_test), np.max(x_test)]\n",
    "y_line = x_line*reg_noisy.coef_ + reg_noisy.intercept_\n",
    "\n",
    "plt.hlines(mean_y, xmin=np.min(x_test), xmax=np.max(x_test));\n",
    "plt.vlines(x_test, ymin=mean_y, ymax=y_test, color=sns.color_palette()[1]);\n",
    "plt.vlines(x_test, ymin=y_test, ymax=y_test_hat, color=sns.color_palette()[4]);\n",
    "sns.scatterplot(x=x_test.squeeze(), y=y_test, color=sns.color_palette()[2], s=50);\n",
    "sns.lineplot(x=x_line, y=y_line, color=sns.color_palette()[1]);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fraction of variance unexplained** is the ratio of the sum of squared\n",
    "distances from data to the regression line (sum of squared vertical\n",
    "distances in second plot), to the sum of squared distanced from data to\n",
    "the mean (sum of squared vertical distances in first plot):\n",
    "\n",
    "$$\\frac{MSE}{Var(y)} = \\frac{Var(y-\\hat{y})}{Var(y)} = \\frac{\\sum_{i=1}^n(y_i-\\hat y_i)^2}{\\sum_{i=1}^n(y_i - \\bar{y})^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative interpretation: imagine we would develop a very simple ML\n",
    "model, in which we always predict $\\hat{y}_i = \\bar{y}_i$. Then, we use\n",
    "this model as a basis for comparison for other, more sophisticated\n",
    "models. The ratio above is the ratio of error of the regression model,\n",
    "to the error of a “prediction by mean” model.\n",
    "\n",
    "-   If this quantity is less than 1, our model is better than\n",
    "    “prediction by mean”\n",
    "-   If this quantity is greater than 1, our model is worse than\n",
    "    “prediction by mean”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fvu = mse_noisy/var_y\n",
    "fvu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = 1 - fvu\n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to do the same thing...\n",
    "metrics.r2_score(y_test, y_test_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does a negative R2 mean, in terms of a comparison to “prediction by\n",
    "mean”?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on coefficient value, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(sigma = widgets.IntSlider(min=0, max=5, step=1, value=2),\n",
    "   coef = widgets.IntSlider(min=-5, max=5, step=1, value=2))\n",
    "def plot_reg(sigma, coef):\n",
    "    x_train, y_train = generate_linear_regression_data(n=20000, d=1, coef=coef, intercept=intercept, sigma=sigma)\n",
    "    x_test,  y_test =  generate_linear_regression_data(n=10000, d=1, coef=coef, intercept=intercept, sigma=sigma)\n",
    "    r_mod = LinearRegression().fit(x_train, y_train)\n",
    "    r2_test = r_mod.score(x_test, y_test)\n",
    "    mse_test = metrics.mean_squared_error(y_test, r_mod.predict(x_test))\n",
    "    x_line = np.array([-3, 3])\n",
    "    y_line = r_mod.predict(x_line.reshape(-1,1))\n",
    "    plt.axhline(y=np.mean(y_train), color=sns.color_palette()[1]);\n",
    "    sns.lineplot(x=x_line, y=y_line, color=sns.color_palette()[2]);\n",
    "    sns.scatterplot(x=x_test[:100,].squeeze(), y=y_test[:100], s=50);\n",
    "    plt.xlabel('x');\n",
    "    plt.ylabel('y');\n",
    "    plt.ylim(-20,20)\n",
    "    plt.xlim(-3,3)\n",
    "    plt.title(\"MSE: %f\\nR2: %f\" % (mse_test, r2_test) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that in this data, the *only* source of error is the $\\epsilon$\n",
    "in\n",
    "\n",
    "$$y_i = w_0 + w_1 x_{i,1} + \\ldots + w_d x_{i,d} + \\epsilon_i $$\n",
    "\n",
    "where $\\epsilon_i \\sim N(0, \\sigma^2)$. If not for this, our regression\n",
    "would fit the data perfectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the coefficient:\n",
    "\n",
    "> An increase in $x$ of 1 is, on average, associated with an increase in\n",
    "> $y$ of about $w_1$.\n",
    "\n",
    "Note that it does not imply any causal relationship!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting MSE and R2:\n",
    "\n",
    "-   MSE shows us the variance of the data around the regression line\n",
    "    (for data with this specific type of “noise”).\n",
    "-   MSE is a measure of the model error, not relative to any baseline.\n",
    "    We can use it to compare different models on the same dataset (but\n",
    "    not on different datasets).\n",
    "-   R2 tells us what fraction of the variance in the data is “explained”\n",
    "    by the regression line.\n",
    "-   R2 is a measure relative to the “prediction by mean” baseline. (Note\n",
    "    that if “prediction by mean” is already good, even a well fitting\n",
    "    regression line will not have a high R2.)\n",
    "-   Prediction by mean is the same thing as prediction by a line with\n",
    "\n",
    "$$w_0 = \\overline{y}, w_1 = 0$$\n",
    "\n",
    "-   The greater the true $w_1$, the more “wrong” the $w_1 = 0$\n",
    "    “prediction” is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual analysis\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sns.load_dataset(\"anscombe\")\n",
    "df.groupby('dataset').agg({'x': ['count','mean', 'std'], 'y': ['count','mean', 'std']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_i   = df[df['dataset'].eq('I')]\n",
    "data_ii  = df[df['dataset'].eq('II')]\n",
    "data_iii = df[df['dataset'].eq('III')]\n",
    "data_iv  = df[df['dataset'].eq('IV')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_i   = LinearRegression().fit(data_i[['x']],   data_i['y'])\n",
    "reg_ii  = LinearRegression().fit(data_ii[['x']],  data_ii['y'])\n",
    "reg_iii = LinearRegression().fit(data_iii[['x']], data_iii['y'])\n",
    "reg_iv  = LinearRegression().fit(data_iv[['x']],  data_iv['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset I:   \",   reg_i.coef_,   reg_i.intercept_)\n",
    "print(\"Dataset II:  \",  reg_ii.coef_,  reg_ii.intercept_)\n",
    "print(\"Dataset III: \", reg_iii.coef_, reg_iii.intercept_)\n",
    "print(\"Dataset IV:  \",  reg_iv.coef_,  reg_iv.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset I:   \", metrics.r2_score(data_i['y'],  reg_i.predict(data_i[['x']])))\n",
    "print(\"Dataset II:  \", metrics.r2_score(data_ii['y'], reg_ii.predict(data_ii[['x']])))\n",
    "print(\"Dataset III: \", metrics.r2_score(data_iii['y'],reg_iii.predict(data_iii[['x']])))\n",
    "print(\"Dataset IV:  \", metrics.r2_score(data_iv['y'], reg_iv.predict(data_iv[['x']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset I:   \", metrics.mean_squared_error(data_i['y'],  reg_i.predict(data_i[['x']])))\n",
    "print(\"Dataset II:  \", metrics.mean_squared_error(data_ii['y'], reg_ii.predict(data_ii[['x']])))\n",
    "print(\"Dataset III: \", metrics.mean_squared_error(data_iii['y'],reg_iii.predict(data_iii[['x']])))\n",
    "print(\"Dataset IV:  \", metrics.mean_squared_error(data_iv['y'], reg_iv.predict(data_iv[['x']])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these models are equally “good” according to our scoring metrics…\n",
    "BUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"x\", y=\"y\", col=\"dataset\", hue=\"dataset\", \n",
    "           data=df, col_wrap=2, ci=None, palette=\"muted\", height=4, \n",
    "           scatter_kws={\"s\": 50, \"alpha\": 1});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the linear model fit well?\n",
    "\n",
    "-   the linear model is a good fit for Dataset I\n",
    "-   Dataset II is clearly non-linear\n",
    "-   Dataset III has an outlier\n",
    "-   Dataset IV has a high leverage point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy to identify problems in 1D - what about in higher D?\n",
    "\n",
    "-   Plot $\\hat{y}$ against $y$\n",
    "-   Plot residuals against $\\hat{y}$\n",
    "-   Plot residuals against each $x$ (including any $x$ not in the model)\n",
    "-   Plot residuals against time (for time series data)\n",
    "\n",
    "What should each of these plots look like if the regression is “good”?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_i   = data_i.assign(   yhat = reg_i.predict(  data_i[['x']]) )\n",
    "data_ii  = data_ii.assign(  yhat = reg_ii.predict( data_ii[['x']]) )\n",
    "data_iii = data_iii.assign( yhat = reg_iii.predict( data_iii[['x']]) )\n",
    "data_iv  = data_iv.assign(  yhat = reg_iv.predict(  data_iv[['x']]) )\n",
    "\n",
    "data_i   = data_i.assign(   residual = data_i['y'] - data_i['yhat'] )\n",
    "data_ii  = data_ii.assign(  residual = data_ii['y'] - data_ii['yhat'] )\n",
    "data_iii = data_iii.assign( residual = data_iii['y'] - data_iii['yhat'] )\n",
    "data_iv  = data_iv.assign(  residual = data_iv['y'] - data_iv['yhat'] )\n",
    "\n",
    "data_all = pd.concat([data_i, data_ii, data_iii, data_iv])\n",
    "data_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"x\", y=\"residual\", col=\"dataset\", hue=\"dataset\", \n",
    "           data=data_all, col_wrap=2, ci=None, palette=\"muted\", height=4, \n",
    "           scatter_kws={\"s\": 50, \"alpha\": 1}, fit_reg=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple linear regression\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = generate_linear_regression_data(n=n_samples, d=2, coef=[5,5], intercept=intercept)\n",
    "x_test,  y_test  = generate_linear_regression_data(n=50, d=2, coef=[5,5], intercept=intercept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5));\n",
    "plt.subplot(1,2,1);\n",
    "plt.scatter(x_train[:,0],  y_train);\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"y\");\n",
    "plt.subplot(1,2,2);\n",
    "plt.scatter(x_train[:,1],  y_train);\n",
    "plt.xlabel(\"x2\");\n",
    "plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that there is no stochastic noise in this data - so it fits a\n",
    "linear model perfectly. But it’s more difficult to see that linear\n",
    "relationship in higher dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_multi = LinearRegression().fit(x_train, y_train)\n",
    "print(\"Coefficient list: \", reg_multi.coef_)\n",
    "print(\"Intercept: \" , reg_multi.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot hyperplane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3D(elev=20, azim=-20, X=x_train, y=y_train):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    ax = plt.subplot(projection='3d')\n",
    "\n",
    "\n",
    "    X1 = np.arange(-4, 4, 0.2)\n",
    "    X2 = np.arange(-4, 4, 0.2)\n",
    "    X1, X2 = np.meshgrid(X1, X2)\n",
    "    Z = X1*reg_multi.coef_[0] + X2*reg_multi.coef_[1]\n",
    "\n",
    "    # Plot the surface.\n",
    "    ax.plot_surface(X1, X2, Z, alpha=0.1, color='gray',\n",
    "                          linewidth=0, antialiased=False)\n",
    "    ax.scatter3D(X[:, 0], X[:, 1], y, s=50)\n",
    "\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.set_zlabel('y')\n",
    "\n",
    "interact(plot_3D, elev=widgets.IntSlider(min=-90, max=90, step=10, value=20), \n",
    "          azim=widgets.IntSlider(min=-90, max=90, step=10, value=20),\n",
    "         X=fixed(x_train), y=fixed(y_train));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = np.arange(3.0, 7.0, 0.05)\n",
    "\n",
    "coef_grid = np.array(np.meshgrid(coefs, coefs)).reshape(1, 2, coefs.shape[0], coefs.shape[0])\n",
    "y_train_hat_c = (reg_multi.intercept_ + np.sum(coef_grid * x_train.reshape(x_train.shape[0], 2, 1, 1), axis=1) )\n",
    "mses_train = np.mean((y_train_hat_c- y_train.reshape(-1, 1, 1))**2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5));\n",
    "p = plt.scatter(x=reg_multi.coef_[0], y=reg_multi.coef_[1], c=sns.color_palette()[1])\n",
    "p = plt.contour(coef_grid[0, 0, :, :], coef_grid[0, 1, :, :], mses_train, levels=5);\n",
    "plt.clabel(p, inline=1, fontsize=10);\n",
    "plt.xlabel('w1');\n",
    "plt.ylabel('w2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = np.arange(3.0, 7.0, 0.05)\n",
    "\n",
    "coef_grid = np.array(np.meshgrid(coefs, coefs)).reshape(1, 2, coefs.shape[0], coefs.shape[0])\n",
    "y_test_hat_c = (reg_multi.intercept_ + np.sum(coef_grid * x_test.reshape(x_test.shape[0], 2, 1, 1), axis=1) )\n",
    "mses_test = np.mean((y_test_hat_c- y_test.reshape(-1, 1, 1))**2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5));\n",
    "p = plt.scatter(x=reg_multi.coef_[0], y=reg_multi.coef_[1], c=sns.color_palette()[1])\n",
    "p = plt.contour(coef_grid[0, 0, :, :], coef_grid[0, 1, :, :], mses_test, levels=5);\n",
    "plt.clabel(p, inline=1, fontsize=10);\n",
    "plt.xlabel('w1');\n",
    "plt.ylabel('w2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple linear regression with noise\n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = generate_linear_regression_data(n=n_samples, d=2, coef=[5,5], intercept=intercept, sigma=5)\n",
    "x_test,  y_test  = generate_linear_regression_data(n=50, d=2, coef=[5,5], intercept=intercept, sigma=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5));\n",
    "plt.subplot(1,2,1);\n",
    "plt.scatter(x_train[:,0],  y_train);\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"y\");\n",
    "plt.subplot(1,2,2);\n",
    "plt.scatter(x_train[:,1],  y_train);\n",
    "plt.xlabel(\"x2\");\n",
    "plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_multi_noisy = LinearRegression().fit(x_train, y_train)\n",
    "print(\"Coefficient list: \", reg_multi_noisy.coef_)\n",
    "print(\"Intercept: \" , reg_multi_noisy.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot hyperplane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3D(elev=20, azim=-20, X=x_train, y=y_train):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    ax = plt.subplot(projection='3d')\n",
    "\n",
    "\n",
    "    X1 = np.arange(-4, 4, 0.2)\n",
    "    X2 = np.arange(-4, 4, 0.2)\n",
    "    X1, X2 = np.meshgrid(X1, X2)\n",
    "    Z = X1*reg_multi_noisy.coef_[0] + X2*reg_multi_noisy.coef_[1]\n",
    "\n",
    "    # Plot the surface.\n",
    "    ax.plot_surface(X1, X2, Z, alpha=0.1, color='gray',\n",
    "                          linewidth=0, antialiased=False)\n",
    "    ax.scatter3D(X[:, 0], X[:, 1], y, s=50)\n",
    "\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.set_zlabel('y')\n",
    "\n",
    "interact(plot_3D, elev=widgets.IntSlider(min=-90, max=90, step=10, value=20), \n",
    "          azim=widgets.IntSlider(min=-90, max=90, step=10, value=20),\n",
    "         X=fixed(x_train), y=fixed(y_train));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = np.arange(3.0, 7.0, 0.05)\n",
    "\n",
    "coef_grid = np.array(np.meshgrid(coefs, coefs)).reshape(1, 2, coefs.shape[0], coefs.shape[0])\n",
    "y_train_hat_c = (reg_multi_noisy.intercept_ + np.sum(coef_grid * x_train.reshape(x_train.shape[0], 2, 1, 1), axis=1) )\n",
    "mses_train = np.mean((y_train_hat_c- y_train.reshape(-1, 1, 1))**2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5));\n",
    "p = plt.scatter(x=reg_multi_noisy.coef_[0], y=reg_multi_noisy.coef_[1], c=sns.color_palette()[1])\n",
    "p = plt.contour(coef_grid[0, 0, :, :], coef_grid[0, 1, :, :], mses_train, levels=5);\n",
    "plt.clabel(p, inline=1, fontsize=10);\n",
    "plt.xlabel('w1');\n",
    "plt.ylabel('w2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = np.arange(3.0, 7.0, 0.05)\n",
    "\n",
    "coef_grid = np.array(np.meshgrid(coefs, coefs)).reshape(1, 2, coefs.shape[0], coefs.shape[0])\n",
    "y_test_hat_c = (reg_multi_noisy.intercept_ + np.sum(coef_grid * x_test.reshape(x_test.shape[0], 2, 1, 1), axis=1) )\n",
    "mses_test = np.mean((y_test_hat_c- y_test.reshape(-1, 1, 1))**2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5));\n",
    "p = plt.scatter(x=reg_multi_noisy.coef_[0], y=reg_multi_noisy.coef_[1], c=sns.color_palette()[1])\n",
    "p = plt.contour(coef_grid[0, 0, :, :], coef_grid[0, 1, :, :], mses_test, levels=5);\n",
    "plt.clabel(p, inline=1, fontsize=10);\n",
    "plt.xlabel('w1');\n",
    "plt.ylabel('w2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear basis function regression\n",
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assumptions of the linear model (that the target variable can be\n",
    "predicted as a linear combination of the features) can be restrictive.\n",
    "We can capture more complicated relationships using linear basis\n",
    "function regression.\n",
    "\n",
    "Fundamental idea: with a set of “basis” functions, we represent the\n",
    "“shape” of our data as a weighted sum of basis functions:\n",
    "\n",
    "$$ \\hat{y_i} =  \\sum_{j=0}^p w_p \\phi_p(\\mathbf{x_i}) $$\n",
    "\n",
    "(We’ll revisit this idea again later in the semester, when we talk about\n",
    "kernels; and again, when we talk about activation functions in neural\n",
    "networks.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re going to look at some examples of basis functions (but not an\n",
    "exhaustive list…)\n",
    "\n",
    "(Note: it’s also possible to mix-and-match basis functions from\n",
    "different “families” in the same model! And, you can apply basis\n",
    "functions to multiple features, too.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear basis\n",
    "\n",
    "Transform a feature $x$ using $\\phi_0(x) = 1, \\phi_1(x) = x$, i.e.\n",
    "\n",
    "$$y \\approx w_0 + w_1 x $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_basis(x):\n",
    "  return np.hstack([np.ones(x.shape),x])\n",
    "\n",
    "x = np.arange(-1.5,1.5,step=0.01).reshape(-1,1)\n",
    "x_trans = linear_basis(x)\n",
    "\n",
    "@interact(w0 = widgets.FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          w1 = widgets.FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          show_sum = False)\n",
    "def plot_linear(w0, w1, show_sum):\n",
    "  plt.figure(figsize=(10,5));\n",
    "  w = np.array([w0, w1])\n",
    "  l = ['1', 'x']\n",
    "  y = np.sum(w*x_trans, axis=1)\n",
    "  if show_sum:\n",
    "    sns.lineplot(x=x.squeeze(), y=y, label='sum', alpha=1, lw=2);\n",
    "  for i in range(2):\n",
    "    sns.lineplot(x=x.squeeze(), y=w[i]*x_trans[:,i], label='$' + l[i] + '$', alpha=0.5);\n",
    "  plt.ylim(-2, 2);\n",
    "  plt.title(\"Linear basis\");\n",
    "  plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0);\n",
    "  plt.xlabel('x')\n",
    "  plt.ylabel('$\\phi(x)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial basis\n",
    "\n",
    "Transform a feature $x$ using $\\phi_j(x) = x^j$, i.e.\n",
    "\n",
    "$$y \\approx w_0 x^0 + w_1 x^1 + \\ldots + w_p x^p $$\n",
    "\n",
    "Note that the model is linear in the parameters $\\mathbf{w}$, which is\n",
    "what makes it a linear model even though it is not linear in $x$.\n",
    "\n",
    "Issue: polynomials are “global” functions - affect the entire range from\n",
    "$-\\infty$ to $\\infty$, and changes very quickly outside the range\n",
    "$[-1,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_basis(x, d):\n",
    "  return np.hstack([x**i for i in range(d)])\n",
    "\n",
    "x = np.arange(-1.5,1.5,step=0.01).reshape(-1,1)\n",
    "x_trans = polynomial_basis(x,5)\n",
    "\n",
    "@interact(w0 = widgets.FloatSlider(min=-1, max=2, step=0.1, value=1),\n",
    "          w1 = widgets.FloatSlider(min=-1, max=2, step=0.1, value=1),\n",
    "          w2 = widgets.FloatSlider(min=-1, max=2, step=0.1, value=1),\n",
    "          w3 = widgets.FloatSlider(min=-1, max=2, step=0.1, value=1),\n",
    "          w4 = widgets.FloatSlider(min=-1, max=2, step=0.1, value=1),\n",
    "          show_sum = False)\n",
    "def plot_poly(w0, w1, w2, w3, w4, show_sum):\n",
    "  plt.figure(figsize=(10,5));\n",
    "  w = np.array([w0, w1, w2, w3, w4])\n",
    "  y = np.sum(w*x_trans, axis=1)\n",
    "  if show_sum:\n",
    "    sns.lineplot(x=x.squeeze(), y=y, label='sum', alpha=1, lw=2);\n",
    "  for i in range(5):\n",
    "    sns.lineplot(x=x.squeeze(), y=w[i]*x_trans[:,i], label='$x^' + str(i) + '$', alpha=0.5);\n",
    "  plt.ylim(-2, 2);\n",
    "  plt.title(\"Polynomial basis\");\n",
    "  plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0);\n",
    "  plt.xlabel('x')\n",
    "  plt.ylabel('$\\phi(x)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial basis\n",
    "\n",
    "Transform a feature $x$ using\n",
    "$\\phi_j(x) = \\exp\\left(-\\frac{(x-\\mu_j)^2}{s^2}\\right)$, i.e.\n",
    "\n",
    "$$y \\approx w_0 \\exp\\left(-\\frac{(x-\\mu_0)^2}{s^2}\\right) + w_1 \\exp\\left(-\\frac{(x-\\mu_1)^2}{s^2}\\right) + \\ldots + w_p \\exp\\left(-\\frac{(x-\\mu_p)^2}{s^2}\\right)$$\n",
    "\n",
    "The model is linear in the parameters $\\mathbf{w}$, which is what makes\n",
    "it a linear model even though it is not linear in $x$. However, it is\n",
    "not linear in the basis function parameters $\\mu_j$ or $s$! (Those basis\n",
    "function parameters will not be “learned” - they are fixed by you.)\n",
    "\n",
    "Note that in contrast to the polynomials which had “global” effect, each\n",
    "radial basis function has “local” effect. (Think of it as a weighted sum\n",
    "of little “bumps”.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0.1\n",
    "def radial_basis(x, mu_list):\n",
    "  return np.hstack([ np.exp(-1*(x-mu)**2/s**2) for mu in mu_list])\n",
    "  \n",
    "x = np.arange(-1.5,1.5,step=0.01).reshape(-1,1)\n",
    "x_trans = radial_basis(x, [-1, -0.5, 0, 0.5, 1])\n",
    "\n",
    "@interact(w0 = widgets.FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          w1 = widgets.FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          w2 = widgets.FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          w3 = widgets.FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          w4 = widgets.FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          show_sum = False)\n",
    "def plot_radial(w0, w1, w2, w3, w4, show_sum):\n",
    "  plt.figure(figsize=(10,5));\n",
    "  w = np.array([w0, w1, w2, w3, w4])\n",
    "  labels = ['$exp(-(x+1)^2)/(' + str(s) + '^2))$', '$exp(-(x+0.5)^2)/(' + str(s) + '^2))$',\n",
    "            '$exp(-(x)^2)/(' + str(s) + '^2))$', '$exp(-(x-0.5)^2)/(' + str(s) + '^2))$',\n",
    "            '$exp(-(x-1)^2)/(' + str(s) + '^2))$']\n",
    "  y = np.sum(w*x_trans, axis=1)\n",
    "  if show_sum:\n",
    "    sns.lineplot(x=x.squeeze(), y=y, label='sum', alpha=1, lw=2);\n",
    "  for i in range(5):\n",
    "    sns.lineplot(x=x.squeeze(), y=w[i]*x_trans[:,i], label=labels[i], alpha=0.5);\n",
    "  plt.ylim(-2, 2)\n",
    "  plt.title(\"Radial basis\");\n",
    "  plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0);\n",
    "  plt.xlabel('x')\n",
    "  plt.ylabel('$\\phi(x)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoidal basis\n",
    "\n",
    "Transform a feature $x$ using \\$\\_j(x) = ( ) \\$ where\n",
    "$\\sigma(a) = \\frac{1}{1+\\exp({-a})}$, i.e.\n",
    "\n",
    "$$y \\approx w_0 \\sigma \\left( \\frac{(x-\\mu_0)}{s}  \\right) + w_1 \\sigma \\left( \\frac{(x-\\mu_1)}{s}  \\right) + \\ldots + w_p \\sigma \\left( \\frac{(x-\\mu_p)}{s}  \\right)$$\n",
    "\n",
    "(Similar to the RBF but with “steps” instead of “bumps”…)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0.05\n",
    "def sigmoid_basis(x, mu_list):\n",
    "  return np.hstack([((1+np.exp((-x-mu)/s))**-1) for mu in mu_list])\n",
    "  \n",
    "x = np.arange(-1.5,1.5,step=0.01).reshape(-1,1)\n",
    "x_trans = sigmoid_basis(x, [-1, -0.5, 0, 0.5, 1])\n",
    "\n",
    "@interact(w0 = widgets.FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          w1 = widgets.FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          w2 = widgets.FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          w3 = widgets.FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          w4 = widgets.FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          show_sum = False)\n",
    "def plot_sigmoid(w0, w1, w2, w3, w4, show_sum):\n",
    "  plt.figure(figsize=(10,5));\n",
    "  w = np.array([w0, w1, w2, w3, w4])\n",
    "  labels = ['$\\sigma((x+1)/' + str(s) + ')$', '$\\sigma((x+0.5)/' + str(s) + ')$', \n",
    "            '$\\sigma((x)/' + str(s) + ')$', '$\\sigma((x-0.5)/' + str(s) + ')$', \n",
    "            '$\\sigma((x-1)/' + str(s) + ')$']\n",
    "  y = np.sum(w*x_trans, axis=1)\n",
    "  if show_sum:\n",
    "    sns.lineplot(x=x.squeeze(), y=y, label='sum', alpha=1, lw=2);\n",
    "  for i in range(5):\n",
    "    sns.lineplot(x=x.squeeze(), y=w[i]*x_trans[:,i], label=labels[i], alpha=0.5);\n",
    "  plt.ylim(-1.5, 2)\n",
    "  plt.title(\"Sigmoidal basis\");\n",
    "  plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0);\n",
    "  plt.xlabel('x')\n",
    "  plt.ylabel('$\\phi(x)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourier basis\n",
    "\n",
    "Transform a feature $x$ using $\\phi_j(x) = cos(\\pi j x) + sin(\\pi j x)$,\n",
    "i.e.\n",
    "\n",
    "$$y \\approx w_0 + w_1 \\sin(\\pi x) + w_2 \\cos(\\pi x) + w_3 \\sin(\\pi  2 x) + w_4 \\cos(\\pi  2 x) + \\ldots $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_basis(x, d):\n",
    "  sins = np.hstack([np.sin(np.pi*i*x) for i in range(1,d+1)])\n",
    "  coss = np.hstack([np.cos(np.pi*i*x) for i in range(1,d+1)])\n",
    "  return np.hstack([sins, coss])\n",
    "    \n",
    "x = np.arange(-1.5,1.5,step=0.01).reshape(-1,1)\n",
    "x_trans = fourier_basis(x, 2)\n",
    "\n",
    "@interact(w1 = widgets.FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          w2 = widgets.FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          w3 = widgets.FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          w4 = widgets.FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          show_sum = False)\n",
    "def plot_fourier(w1, w2, w3, w4, show_sum):\n",
    "  plt.figure(figsize=(10,5));\n",
    "  w = np.array([w1, w2, w3, w4])\n",
    "  labels = [\"cos(\\pi x)\", \"sin(\\pi x)\", \"cos(\\pi 2 x)\", \"sin(\\pi 2 x)\"]\n",
    "  y = np.sum(w*x_trans, axis=1)\n",
    "  if show_sum:\n",
    "    sns.lineplot(x=x.squeeze(), y=y, label='sum', alpha=1, lw=2);\n",
    "  for i in range(4):\n",
    "    sns.lineplot(x=x.squeeze(), y=w[i]*x_trans[:,i], label='$' + labels[i] + '$', alpha=0.5);\n",
    "  plt.ylim(-1.5, 2)\n",
    "  plt.title(\"Fourier basis\");\n",
    "  plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0);\n",
    "  plt.xlabel('x')\n",
    "  plt.ylabel('$\\phi(x)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s consider an example! Now suppose we have a process that generates\n",
    "data as\n",
    "\n",
    "$$y_i = w_0 + w_1 x_{i,1} + w_2 x_{i,2} + w_3 x_{i,1}^2 + w_4 x_{i,2}^2 + w_5 x_{i,1} x_{i,2} + \\epsilon_i $$\n",
    "\n",
    "where $\\epsilon_i \\sim N(0, \\sigma^2)$. In other words:\n",
    "\n",
    "$$\\mathbf{\\phi} = [1, x_1, x_2, x_1^2, x_2^2, x_1 x_2] $$\n",
    "\n",
    "Note that the model is *linear* in $\\textbf{w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def generate_linear_basis_data(n=200, d=2, coef=[1,1,0.5,0.5,1], intercept=1, sigma=0):\n",
    "  x = np.random.randn(n,d)\n",
    "  x = np.column_stack((x, x**2 ))\n",
    "  for pair in list(itertools.combinations(range(d), 2)):\n",
    "    x = np.column_stack((x, x[:,pair[0]]*x[:,pair[1]]))\n",
    "  y = (np.dot(x, coef) + intercept).squeeze() + sigma * np.random.randn(n)\n",
    "  return x[:,:d], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = generate_linear_basis_data(sigma=0.2)\n",
    "x_test,  y_test  = generate_linear_basis_data(n=50, sigma=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5));\n",
    "plt.subplot(1,2,1);\n",
    "plt.scatter(x_train[:,0],  y_train);\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"y\");\n",
    "plt.subplot(1,2,2);\n",
    "plt.scatter(x_train[:,1],  y_train);\n",
    "plt.xlabel(\"x2\");\n",
    "plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3D(elev, azim, w0, w1, w2, w3, w4, w5, show_sum, show_basis, show_data, X, y):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    ax = plt.subplot(projection='3d')\n",
    "\n",
    "    X1 = np.arange(-4, 4, 0.2)\n",
    "    X2 = np.arange(-4, 4, 0.2)\n",
    "    X1, X2 = np.meshgrid(X1, X2)\n",
    "    Z0 = w0*np.ones(shape=(X1.shape[0], X2.shape[0]))\n",
    "    Z1 = w1*X1\n",
    "    Z2 = w2*X2\n",
    "    Z3 = w3*X1**2\n",
    "    Z4 = w4*X2**2\n",
    "    Z5 = w5*X1*X2\n",
    "\n",
    "    # Plot the surfaces.\n",
    "    if show_basis:\n",
    "      ax.plot_surface(X1, X2, Z0, alpha=0.1, color=sns.color_palette()[1], linewidth=0, antialiased=False)\n",
    "      ax.plot_surface(X1, X2, Z1, alpha=0.1, color=sns.color_palette()[2], linewidth=0, antialiased=False)\n",
    "      ax.plot_surface(X1, X2, Z2, alpha=0.1, color=sns.color_palette()[3], linewidth=0, antialiased=False)\n",
    "      ax.plot_surface(X1, X2, Z3, alpha=0.1, color=sns.color_palette()[4], linewidth=0, antialiased=False)\n",
    "      ax.plot_surface(X1, X2, Z4, alpha=0.1, color=sns.color_palette()[5], linewidth=0, antialiased=False)\n",
    "      ax.plot_surface(X1, X2, Z5, alpha=0.1, color=sns.color_palette()[6], linewidth=0, antialiased=False)\n",
    "    if show_sum:\n",
    "      ax.plot_surface(X1, X2, (Z0+Z1+Z2+Z3+Z4+Z5), alpha=0.5, color='white', linewidth=0, antialiased=False)\n",
    "    if show_data:\n",
    "      ax.scatter3D(X[:, 0], X[:, 1], y, s=50)\n",
    "\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.set_zlabel('y')\n",
    "    ax.set_zlim(0, 25)\n",
    "\n",
    "interact(plot_3D, elev=widgets.IntSlider(min=-90, max=90, step=10, value=20), \n",
    "          azim=widgets.IntSlider(min=-90, max=90, step=10, value=20),\n",
    "          w0 = widgets.FloatSlider(min=-1, max=2, step=0.1, value=1),\n",
    "          w1 = widgets.FloatSlider(min=-1, max=2, step=0.1, value=1),\n",
    "          w2 = widgets.FloatSlider(min=-1, max=2, step=0.1, value=1),\n",
    "          w3 = widgets.FloatSlider(min=-1, max=2, step=0.1, value=0.5),\n",
    "          w4 = widgets.FloatSlider(min=-1, max=2, step=0.1, value=0.5),\n",
    "          w5 = widgets.FloatSlider(min=-1, max=2, step=0.1, value=1),\n",
    "         show_sum = False, show_basis = False, show_data = False,\n",
    "         X=fixed(x_train), y=fixed(y_train));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_lbf = LinearRegression().fit(x_train, y_train)\n",
    "print(\"Intercept: \" , reg_lbf.intercept_)\n",
    "print(\"Coefficient list: \", reg_lbf.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute MSE and R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_hat = reg_lbf.predict(x_train)\n",
    "print(\"Training MSE: \", metrics.mean_squared_error(y_train, y_train_hat))\n",
    "print(\"Training R2:  \", metrics.r2_score(y_train, y_train_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot hyperplane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3D(elev=20, azim=-20, X=x_train, y=y_train):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    ax = plt.subplot(projection='3d')\n",
    "\n",
    "\n",
    "    X1 = np.arange(-4, 4, 0.2)\n",
    "    X2 = np.arange(-4, 4, 0.2)\n",
    "    X1, X2 = np.meshgrid(X1, X2)\n",
    "    Z = X1*reg_lbf.coef_[0] + X2*reg_lbf.coef_[1] + reg_lbf.intercept_\n",
    "\n",
    "    # Plot the surface.\n",
    "    ax.plot_surface(X1, X2, Z, alpha=0.1, color='gray',\n",
    "                          linewidth=0, antialiased=False)\n",
    "    ax.scatter3D(X[:, 0], X[:, 1], y, s=50)\n",
    "\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.set_zlabel('y')\n",
    "\n",
    "interact(plot_3D, elev=widgets.IntSlider(min=-90, max=90, step=10, value=20), \n",
    "          azim=widgets.IntSlider(min=-90, max=90, step=10, value=20),\n",
    "         X=fixed(x_train), y=fixed(y_train));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_train = y_train - y_train_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim_hi = np.max(np.concatenate([y_train, y_train_hat]))\n",
    "lim_lo = np.min(np.concatenate([y_train, y_train_hat]))\n",
    "sns.scatterplot(x=y_train, y=y_train_hat);\n",
    "plt.xlabel('y');\n",
    "plt.ylabel('y_hat');\n",
    "plt.xlim(lim_lo, lim_hi);\n",
    "plt.ylim(lim_lo, lim_hi);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the error random? Or does it look systematic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=y_train_hat, y=residual_train);\n",
    "plt.xlabel('y_hat');\n",
    "plt.ylabel('Residual');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5));\n",
    "plt.subplot(1,2,1);\n",
    "plt.scatter(x_train[:,0],  residual_train);\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"Residual\");\n",
    "plt.subplot(1,2,2);\n",
    "plt.scatter(x_train[:,1],  residual_train);\n",
    "plt.xlabel(\"x2\");\n",
    "plt.ylabel(\"Residual\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is clearly some non-linearity, we can try to fit a model to\n",
    "a non-linear transformation of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_trans = np.column_stack((x_train, x_train**2))\n",
    "\n",
    "reg_lbf_trans = LinearRegression().fit(x_train_trans, y_train)\n",
    "print(\"Intercept: \" , reg_lbf_trans.intercept_)\n",
    "print(\"Coefficient list: \", reg_lbf_trans.coef_)\n",
    "\n",
    "y_train_trans_hat = reg_lbf_trans.predict(x_train_trans)\n",
    "print(\"Training MSE: \", metrics.mean_squared_error(y_train, y_train_trans_hat))\n",
    "print(\"Training R2:  \", metrics.r2_score(y_train, y_train_trans_hat))\n",
    "\n",
    "residual_train_trans = y_train - y_train_trans_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim_hi = np.max(np.concatenate([y_train, y_train_trans_hat]))\n",
    "lim_lo = np.min(np.concatenate([y_train, y_train_trans_hat]))\n",
    "sns.scatterplot(x=y_train, y=y_train_trans_hat);\n",
    "plt.xlabel('y');\n",
    "plt.ylabel('y_hat');\n",
    "plt.xlim(lim_lo, lim_hi);\n",
    "plt.ylim(lim_lo, lim_hi);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=y_train, y=residual_train_trans);\n",
    "plt.xlabel('y');\n",
    "plt.ylabel('Residual');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5));\n",
    "plt.subplot(1,2,1);\n",
    "plt.scatter(x_train[:,0],  residual_train_trans);\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"Residual\");\n",
    "plt.subplot(1,2,2);\n",
    "plt.scatter(x_train[:,1],  residual_train_trans);\n",
    "plt.xlabel(\"x2\");\n",
    "plt.ylabel(\"Residual\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_inter = np.column_stack((x_train_trans, x_train[:,0]*x_train[:,1]))\n",
    "\n",
    "reg_lbf_inter = LinearRegression().fit(x_train_inter, y_train)\n",
    "print(\"Intercept: \" , reg_lbf_inter.intercept_)\n",
    "print(\"Coefficient list: \", reg_lbf_inter.coef_)\n",
    "\n",
    "y_train_inter_hat = reg_lbf_inter.predict(x_train_inter)\n",
    "print(\"Training MSE: \", metrics.mean_squared_error(y_train, y_train_inter_hat))\n",
    "print(\"Training R2:  \", metrics.r2_score(y_train, y_train_inter_hat))\n",
    "\n",
    "residual_train_inter = y_train - y_train_inter_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim_hi = np.max(np.concatenate([y_train, y_train_inter_hat]))\n",
    "lim_lo = np.min(np.concatenate([y_train, y_train_inter_hat]))\n",
    "sns.scatterplot(x=y_train, y=y_train_inter_hat);\n",
    "plt.xlabel('y');\n",
    "plt.ylabel('y_hat');\n",
    "plt.xlim(lim_lo, lim_hi);\n",
    "plt.ylim(lim_lo, lim_hi);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=y_train, y=residual_train_inter);\n",
    "plt.xlabel('y');\n",
    "plt.ylabel('Residual');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5));\n",
    "plt.subplot(1,2,1);\n",
    "plt.scatter(x_train[:,0],  residual_train_inter);\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"Residual\");\n",
    "plt.subplot(1,2,2);\n",
    "plt.scatter(x_train[:,1],  residual_train_inter);\n",
    "plt.xlabel(\"x2\");\n",
    "plt.ylabel(\"Residual\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_hat = reg_lbf_inter.predict(x_train_inter)\n",
    "print(\"Training MSE: \", metrics.mean_squared_error(y_train, y_train_hat))\n",
    "print(\"Training R2:  \", metrics.r2_score(y_train, y_train_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_inter = np.column_stack((x_test, x_test**2))\n",
    "x_test_inter = np.column_stack((x_test_inter, x_test[:,0]*x_test[:,1]))\n",
    "\n",
    "y_test_hat = reg_lbf_inter.predict(x_test_inter)\n",
    "print(\"Test MSE: \", metrics.mean_squared_error(y_test, y_test_hat))\n",
    "print(\"Test R2:  \", metrics.r2_score(y_test, y_test_hat))"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
