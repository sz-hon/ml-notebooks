{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo: Neural network for classification\n",
    "=======================================\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "*This is based on the course of [Fraida Fund](https://colab.research.google.com/github/ffund/ml-notebooks/blob/master/notebooks/1-colab-tour.ipynb) for  NYU Tandon School of Engineering*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<small>Attribution: Some parts of this notebook are written by Sundeep Rangan, from his [IntroML GitHub repo](https://github.com/sdrangan/introml/).</small>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previously considered a general approach to learning a non-linear function of the input data: we can supply a non-linear data representation, by applying a non-linear transformation to the input data, or specifying a non-linear kernel. Then, the machine learning model learns from the transformed data.\n",
    "\n",
    "The power of neural networks is their ability to *learn* a “transformation”, rather than having to specify it ourselves! In this demo, we will see how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw a classification problem\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, draw on the canvas to fill in the region of the feature space that should be part of the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ### Colab drawing magic\n",
    "# colab drawing magic via \n",
    "# https://gist.github.com/korakot/8409b3feec20f159d8a50b0a811d3bca\n",
    "from IPython.display import HTML, Image\n",
    "from google.colab.output import eval_js\n",
    "from base64 import b64decode\n",
    "\n",
    "canvas_html = \"\"\"\n",
    "<canvas width=%d height=%d style='border:1px solid #000000;'></canvas>\n",
    "<button>Finish</button>\n",
    "<script>\n",
    "var canvas = document.querySelector('canvas')\n",
    "var ctx = canvas.getContext('2d')\n",
    "ctx.lineWidth = %d\n",
    "var button = document.querySelector('button')\n",
    "var mouse = {x: 0, y: 0}\n",
    "\n",
    "canvas.addEventListener('mousemove', function(e) {\n",
    "  mouse.x = e.pageX - this.offsetLeft\n",
    "  mouse.y = e.pageY - this.offsetTop\n",
    "})\n",
    "canvas.onmousedown = ()=>{\n",
    "  ctx.beginPath()\n",
    "  ctx.moveTo(mouse.x, mouse.y)\n",
    "  canvas.addEventListener('mousemove', onPaint)\n",
    "}\n",
    "canvas.onmouseup = ()=>{\n",
    "  canvas.removeEventListener('mousemove', onPaint)\n",
    "}\n",
    "var onPaint = ()=>{\n",
    "  ctx.lineTo(mouse.x, mouse.y)\n",
    "  ctx.stroke()\n",
    "}\n",
    "\n",
    "var data = new Promise(resolve=>{\n",
    "  button.onclick = ()=>{\n",
    "    resolve(canvas.toDataURL('image/png'))\n",
    "  }\n",
    "})\n",
    "</script>\n",
    "\"\"\"\n",
    "\n",
    "def draw(filename='drawing.png', w=256, h=256, line_width=10):\n",
    "  display(HTML(canvas_html % (w, h, line_width)))\n",
    "  data = eval_js(\"data\")\n",
    "  binary = b64decode(data.split(',')[1])\n",
    "  with open(filename, 'wb') as f:\n",
    "    f.write(binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "im = Image.open(\"drawing.png\")\n",
    "np_im = np.array(im)\n",
    "print(np_im.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_im_bw = np_im[:,:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(np_im_bw, cmap='binary');\n",
    "plt.xlim(0,255);\n",
    "plt.ylim(255,0);\n",
    "plt.xlabel('x1');\n",
    "plt.ylabel('x2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get training, validation, and test data\n",
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 5000\n",
    "n_val   = 1000\n",
    "n_test  = 10000\n",
    "\n",
    "X_train = np.column_stack((np.random.randint(0, np_im_bw.shape[0], size=n_train), np.random.randint(0, np_im_bw.shape[1], size=n_train)))\n",
    "y_train = np.rot90(np_im_bw, k=3)[X_train[:,0], X_train[:,1]]\n",
    "\n",
    "X_train = X_train/255.0\n",
    "y_train = y_train/255.0\n",
    "\n",
    "\n",
    "X_val = np.column_stack((np.random.randint(0, np_im_bw.shape[0], size=n_val), np.random.randint(0, np_im_bw.shape[1], size=n_val)))\n",
    "y_val = np.rot90(np_im_bw, k=3)[X_val[:,0], X_val[:,1]]\n",
    "\n",
    "X_val = X_val/255.0\n",
    "y_val = y_val/255.0\n",
    "\n",
    "\n",
    "X_test = np.column_stack((np.random.randint(0, np_im_bw.shape[0], size=n_test), np.random.randint(0, np_im_bw.shape[1], size=n_test)))\n",
    "y_test = np.rot90(np_im_bw, k=3)[X_test[:,0], X_test[:,1]]\n",
    "\n",
    "X_test = X_test/255.0\n",
    "y_test = y_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.scatter(x=X_train[:,0], y=X_train[:,1], c=y_train, cmap='binary', edgecolors= \"gray\")\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"x2\");\n",
    "plt.title(\"Training data\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Tensorflow + Keras\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demo, we will use Tensorflow with Keras to train and configure a neural network.\n",
    "\n",
    "Tensorflow is a powerful and widely-used platform for deep learning. However, Tensorflow is relatively low level and may be somewhat difficult to use as a starting point. We will use the `keras` package which acts as a high-level wrapper on top of tensorflow that allows you to quickly build and fit models.\n",
    "\n",
    "(A very popular alternative to Tensorflow for working with neural networks is PyTorch - you can read more about PyTorch [here](https://pytorch.org/).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(There are two major versions of Tensorflow. Code that was written for TF1 often will not run with TF2. Fortunately, you can load either version in Colab, as described [here](https://colab.research.google.com/notebooks/tensorflow_version.ipynb).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the neural network\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see if we can build a simple neural network classifier to learn the decision region that we drew above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import some key sub-packages from `keras`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we clear the session. This is not strictly necessary since we haven’t computed anything yet, but it is good practice if you are going to train multiple “temporary” models in a row - it frees up memory associated with models that are no longer in scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a very simple network.\n",
    "\n",
    "The neural network has `nin=2` input units, corresponding to the two features.\n",
    "\n",
    "It has one hidden layer with `nh=4` hidden units. Each hidden unit will use a sigmoid activation function.\n",
    "\n",
    "There is `nout=1` output unit corresponding to the estimated class label. This unit will also have a sigmoid activation function.\n",
    "\n",
    "<small>Reference: [TF+keras activations](https://www.tensorflow.org/api_docs/python/tf/keras/activations).</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the layers are *dense* or *fully connected* layers, meaning that each node has a link to every node in the adjacent layers.\n",
    "\n",
    "<small>Reference: [Keras layers](https://keras.io/api/layers/)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nin = 2  # dimension of input data\n",
    "nh = 4  # number of hidden units\n",
    "nout = 1  # number of outputs = 1 since this is binary\n",
    "model = Sequential()\n",
    "model.add(Dense(units=nh, input_shape=(nin,), activation='sigmoid', name='hidden'))\n",
    "model.add(Dense(units=nout, activation='sigmoid', name='output'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our visualization of the network architechture, we will also add a *bias node* at each layer. This simplifies the computation of weights by adding an extra input whose value is always 1. The bias term then comes from the weight applied to that input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Visualize the network architecture\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "inputLayerSize  = nin\n",
    "outputLayerSize = nout\n",
    "hiddenLayerSize = nh\n",
    "\n",
    "\n",
    "nodePos = {}\n",
    "G=nx.Graph()\n",
    "graphHeight = max(inputLayerSize, outputLayerSize, hiddenLayerSize)\n",
    "\n",
    "# create nodes and note their positions\n",
    "for n in range(inputLayerSize):\n",
    "  nodePos['x'+str(n+1)]=(1, n)\n",
    "  G.add_node('x'+str(n+1))\n",
    "for n in range(outputLayerSize):\n",
    "  nodePos['o'+str(n+1)]=(5, n)\n",
    "  G.add_node('o'+str(n+1))\n",
    "for n in range(hiddenLayerSize):\n",
    "  nodePos['h'+str(n+1)]=(3, n)\n",
    "  G.add_node('h'+str(n+1))\n",
    "\n",
    "# add edges\n",
    "for n in range(hiddenLayerSize):\n",
    "  for m in range(inputLayerSize):\n",
    "    G.add_edge('x' + str(m+1), 'h' + str(n+1))\n",
    "  for m in range(outputLayerSize):\n",
    "    G.add_edge('h' + str(n+1), 'o' + str(m+1))\n",
    "\n",
    "# add bias nodes\n",
    "\n",
    "nodePos['xb']=(1, inputLayerSize)\n",
    "G.add_node('xb')\n",
    "for n in range(hiddenLayerSize):\n",
    "  G.add_edge('xb', 'h' + str(n+1))\n",
    "\n",
    "nodePos['hb']=(3, hiddenLayerSize)\n",
    "G.add_node('hb')\n",
    "for n in range(outputLayerSize):\n",
    "  G.add_edge('hb', 'o' + str(n+1))\n",
    "\n",
    "nx.draw_networkx(G, pos=nodePos, \n",
    "              node_size=1000, node_color='pink')\n",
    "plt.axis('off');\n",
    "plt.margins(0.2, 0.2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each node in the neural network applies its activation function to the weighted sum of its inputs, then passes that as output to the nodes in the next layer.\n",
    "\n",
    "The neural network needs to learn a weight for each link in the image above (including the weights for the bias nodes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras has a nice command for summarizing the network architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to select an optimizer and a loss function.\n",
    "\n",
    "Since this is a binary classification problem, we select the `binary_crossentropy` loss.\n",
    "\n",
    "We will also choose an optimizer (Adam) and a learning rate of `learning_rate=0.01`.\n",
    "\n",
    "We also set the `metrics` that we wish to track during the optimization. In this case, we select `accuracy` on the training set.\n",
    "\n",
    "Finally, before training, we must `compile` the model.\n",
    "\n",
    "<small>Reference: [TF+keras loss functions](https://www.tensorflow.org/api_docs/python/tf/keras/losses), [Keras optimizers](https://keras.io/api/optimizers/)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "opt = optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=opt,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the neural network\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides a simple method `fit` to run the optimization. You simply specify the number of epochs and the batch size.\n",
    "\n",
    "We will also pass it some `validation_data` - it will also compute the loss and accuracy on the validation data at each epoch.\n",
    "\n",
    "(During one epoch, we run as many iterations of mini-batch stochastic gradient descent as it takes to sample all the training data, without replacement.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(X_train, y_train, epochs=100, batch_size=24, validation_data=(X_val,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the `fit` cell more than once, it will update the fit by running more gradient descent iterations - it won’t start again from the beginning. However, it *will* overwrite your previous model “history” (i.e. the loss and accuracy per epoch from the last call to `fit`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize output of network\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the classification rule our neural network learned, we can plot the predicted class probability over the input space.\n",
    "\n",
    "We can also plot the response in the each of the hidden units.\n",
    "\n",
    "Each of the hidden units produces one linear decision region. The final nonlinear region is then formed by taking a weighted combination of these regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Visualize output of each unit\n",
    "\n",
    "# Prepare coordinate grid\n",
    "n_plot = 256\n",
    "xx1, xx2 = np.meshgrid(np.arange(0, n_plot, 1),np.arange(0, n_plot, 1))\n",
    "X_grid = np.hstack((xx1.reshape(n_plot**2,1), xx2.reshape(n_plot**2,1)))\n",
    "X_grid = X_grid/(n_plot-1.0)\n",
    "\n",
    "# Get the response in the hidden units \n",
    "layer_hid = model.get_layer('hidden')\n",
    "model1 = Model(inputs=model.input,\n",
    "               outputs=layer_hid.output)\n",
    "zhid_plot = model1.predict(X_grid)\n",
    "zhid_plot = zhid_plot.reshape((n_plot,n_plot,nh))\n",
    "\n",
    "# Get the weights in the output layer\n",
    "layer_out = model.get_layer('output')\n",
    "Wo, bo = layer_out.get_weights()\n",
    "\n",
    "# Get the response in the output layer\n",
    "yplot = model.predict(X_grid)\n",
    "yplot_mat = yplot[:,0].reshape((n_plot, n_plot))\n",
    "\n",
    "# Plot the output layer\n",
    "fig = plt.figure(figsize=(15, 6))\n",
    "plt.subplot(1,nh+1,1)\n",
    "plt.imshow(np.flipud((yplot_mat.reshape(n_plot, n_plot))), cmap='binary');\n",
    "plt.xlim(0, n_plot);\n",
    "plt.ylim(n_plot,0);\n",
    "plt.xticks([]);\n",
    "plt.yticks([]);\n",
    "plt.title(\"Output\")\n",
    "plt.xlim(0,n_plot);\n",
    "plt.ylim(n_plot, 0);\n",
    "\n",
    "# Plot the hidden layers\n",
    "for i in range(nh):\n",
    "\n",
    "    plt.subplot(1,nh+1,i+1+1)\n",
    "    zhid_ploti = (zhid_plot[:,:,i])\n",
    "    im = plt.imshow(zhid_ploti, vmin=0, vmax=1, cmap='binary')\n",
    "    im = plt.imshow(np.flipud((zhid_ploti)), cmap='binary');\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title('h{0:d}, Wo={1:4.2f}'.format(i+1,Wo[i,0]))\n",
    "    plt.xlim(0,n_plot);\n",
    "    plt.ylim(n_plot,0);\n",
    "\n",
    "fig.subplots_adjust(right=0.85)\n",
    "cbar_ax = fig.add_axes([0.9, 0.30, 0.05, 0.4])\n",
    "fig.colorbar(im, cax=cbar_ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check all of the parameters learned by the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_hid = model.get_layer('hidden')\n",
    "Wh, bh = layer_hid.get_weights()\n",
    "print('Wh=')\n",
    "print(Wh)\n",
    "print('bh=')\n",
    "print(bh)\n",
    "\n",
    "layer_out = model.get_layer('output')\n",
    "Wo, bo = layer_out.get_weights()\n",
    "print('Wo=')\n",
    "print(Wo)\n",
    "print('bo=')\n",
    "print(bo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, we can visualize the training progress vs. epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(7,3))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "train_acc = hist.history['accuracy'];\n",
    "val_acc = hist.history['val_accuracy'];\n",
    "\n",
    "nepochs = len(train_acc);\n",
    "sns.lineplot(x=np.arange(1,nepochs+1), y=train_acc, label='Training accuracy');\n",
    "sns.lineplot(x=np.arange(1,nepochs+1), y=val_acc, label='Validation accuracy');\n",
    "plt.xlabel('Epoch');\n",
    "plt.ylabel('Accuracy');\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "train_loss = hist.history['loss']\n",
    "val_loss = hist.history['val_loss']\n",
    "sns.lineplot(x=np.arange(1,nepochs+1), y=train_loss, label='Training loss');\n",
    "sns.lineplot(x=np.arange(1,nepochs+1), y=val_loss, label='Validation loss');\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure performance on test set\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.round(y_pred).astype(int)\n",
    "y_pred = y_pred.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((y_pred == y_test).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x=X_test[:,0], y=X_test[:,1], c=y_pred, cmap='binary', edgecolors= \"gray\")\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"x2\");\n",
    "plt.title(\"Predictions for test data\");\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x=X_test[:,0], y=X_test[:,1], c=y_test, cmap='binary', edgecolors= \"gray\")\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"x2\");\n",
    "plt.title(\"Actual test data\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to try\n",
    "-------------\n",
    "\n",
    "-   What happens if we use `linear` activations, instead of sigmoid?\n",
    "-   When do we need to use a large number of hidden units?\n",
    "-   Try to adjust the learning rate and batch size. What is the effect?\n",
    "-   What should you do if you want to train the model for a few more epochs? (What happens to your history when you do that?)\n",
    "-   What should you do if you want to change the model architecture and train it from scratch?"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 }
}
