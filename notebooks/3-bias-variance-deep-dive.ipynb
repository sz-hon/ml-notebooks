{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias/variance and model selection in depth\n",
    "==========================================\n",
    "\n",
    "*This is based on the course of [Fraida Fund](https://colab.research.google.com/github/ffund/ml-notebooks/blob/master/notebooks/1-colab-tour.ipynb) for  NYU Tandon School of Engineering*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attribution\n",
    "\n",
    "Parts of this notebook borrow from, or are inspired by, the following sources:\n",
    "\n",
    "-   The Introduction section is based on [Hyperparameters and Model Validation](https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html), in the Python Data Science Handbook by Jake VanderPlas\n",
    "-   The section on polynomial models is based on [the model order selection demo notebook](https://colab.research.google.com/github/sdrangan/introml/blob/master/unit04_model_sel/demo_polyfit.ipynb) by Prof. Sundeep Rangan, and some of the text in that section is copied from that notebook.\n",
    "-   The section on uninformative features is based on [Cross Validation: The Right and Wrong Way](http://nbviewer.ipython.org/urls/raw.github.com/cs109/content/master/lec_10_cross_val.ipynb) from [Harvard CS109 Data Science](https://github.com/cs109/content)\n",
    "-   The simulation plots are based on the `scikit-learn` example [Single estimator versus bagging: bias-variance decomposition](https://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html#sphx-glr-auto-examples-ensemble-plot-bias-variance-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "------------\n",
    "\n",
    "As a distinguished machine learning expert, you are hired as a consultant to solve a difficult problem for a client. You apply the best <sup>\\[1\\]</sup> machine learning model you know, and present the results to the client. The response?\n",
    "\n",
    "> Not good enough! We expect much better performance.\n",
    "\n",
    "As a machine learning expert, what should you do to improve your results?\n",
    "\n",
    "-   Find and correct a problem with the data (garbage in, garbage out!)\n",
    "-   Find and correct a problem with the model implementation/training\n",
    "-   Use a more complicated model (more flexible)\n",
    "-   Use a less complicated model (less flexible)\n",
    "-   Get more training samples\n",
    "-   Get more data to add features to each sample - for example, you could join two related datasets to get more potentially predictive features\n",
    "-   Add additional features using transformed versions of the features you already have\n",
    "\n",
    "Each of these possible solutions has the potential to improve results, or to have no effect. In some cases, they can actually make the performance even worse.\n",
    "\n",
    "How do you know which one to try?\n",
    "\n",
    "<small>\\[1\\] There is actually no “best” machine learning model in general - various models will perform better on different problems.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first instinct may be to try all of these potential solutions, and see which one improves performance. But that could be very expensive -\n",
    "\n",
    "-   Your time costs \\$ (you are getting paid for consulting on this problem!)\n",
    "-   You may need to use metered computing resources to train the model, which costs \\$\n",
    "-   Collecting data, especially labeled data, often costs \\$\n",
    "-   Your client is losing \\$ due to using a sub-optimal model in production while waiting for you to improve it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a machine learning expert, your knowledge of the theoretical basics of machine learning, and your ability to *connect that knowledge* to practical solutions for real problems, should prepare you for this moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our previous work in this course has prepared us to evaluate some of these “next steps” to see whether they would be appropriate.\n",
    "\n",
    "**Find and correct a problem with the data (garbage in, garbage out!)**:\n",
    "\n",
    "-   We practiced using exploratory data analysis as a first step toward identifying problems or wrong assumptions related to the data.\n",
    "-   We also saw that when using models with good *interpretability*, we can inspect the fitted model as another step toward finding problems with the data. For example, in a case study on linear regression, we saw that the coefficients for certain features were high when common sense dictated that those features should not have any effect on the target variable.\n",
    "\n",
    "**Find and correct a problem with the model implementation/training**:\n",
    "\n",
    "-   We looked at, and will continue to look at, some problems that can arise with model training, for example learning rate too high/too low in gradient descent.\n",
    "\n",
    "**Get more data to add features to each sample**:\n",
    "\n",
    "-   We practiced inspecting the residuals. Plots of residuals against $y$ or against each of the features are a good way to see a 2D view of the results of the model, even when the dimension of the data is large.\n",
    "-   When there appears to be a pattern in the residuals, this indicates something “learnable” in the data that isn’t captured by our model.\n",
    "    -   For example, suppose we find that the residuals appear to be a function of $y$, but not of any feature $x$. This suggests that $y$ is partly a function of a “missing” feature, which we should try to identify (using domain knowledge) and add to our model.\n",
    "    -   Similarly, if the residuals appear to be a function of a feature $x$ that is in our data, but we are not using to train the model, we should absolutely add that feature to our model.\n",
    "-   Another thing to look for in a residuals plot: “outliers” or extreme values. These can sometimes indicate problems with the data,such as samples affected by measurement or recording error. (In this case, you might want to exclude these samples, and also figure out how to exclude them in production.) These can also indicate that the data is drawn from two distributions, where samples from the minority distribution appear as outliers. (In this case, you wouldn’t want to exclude these samples. You would want to add a feature to help you distinguish between the two populations, and then re-train your model with the added feature.)\n",
    "\n",
    "**Add additional features using transformed versions of the features you already have**:\n",
    "\n",
    "-   If the plot of residuals against a feature $x$ appears to have a non-linear pattern, we should consider adding a transformed version of that feature to our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s a good start. But to help us identify whether some of the other steps would be helpful, we need to understand *bias* and *variance*. Then, we’ll be on our way towards understanding when to:\n",
    "\n",
    "-   Use a more complicated model (more flexible)\n",
    "-   Use a less complicated model (less flexible)\n",
    "-   Get more training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting and underfitting: Polynomial data\n",
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the concept of overfitting, we will consider data generated by a polynomial of order $d$, with some added Gaussian noise:\n",
    "\n",
    "$$y_i = w_0 + w_1 x_{1} + \\cdots + w_d x_{i}^d + \\epsilon $$\n",
    "\n",
    "where $\\epsilon_i \\sim N(0, \\sigma^2)$.\n",
    "\n",
    "We can fit this model using a linear basis function regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a function that generates polynomial data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_polynomial_regression_data(n=100, xrange=[-1,1], coefs=[1,0.5,0,2], sigma=0.5):\n",
    "  x = np.random.uniform(xrange[0], xrange[1], n)\n",
    "  y = np.polynomial.polynomial.polyval(x,coefs) + sigma * np.random.randn(n)\n",
    "\n",
    "  return x.reshape(-1,1), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs=[1,0.5,0,2]\n",
    "n_samples = 100\n",
    "sigma = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = generate_polynomial_regression_data(n=n_samples, coefs=coefs, sigma=sigma)\n",
    "x_train, y_train = generate_polynomial_regression_data(n=n_samples, coefs=coefs, sigma=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=x_test.squeeze(), y=y_test, label='Test points');\n",
    "plt.xlabel(\"x\");\n",
    "plt.ylabel(\"y\");\n",
    "plt.title(\"Data generated from $1 + 0.5x + 2x^3$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, since the data is generated synthetically, we know the “true” function that generated the data. We can plot this function along with the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=x_test.squeeze(), y=y_test, label='Test points');\n",
    "plt.xlabel(\"x\");\n",
    "plt.ylabel(\"y\");\n",
    "plt.title(\"Data generated from $1 + 0.5x + 2x^3$\");\n",
    "\n",
    "# Plot true function\n",
    "x_p = np.linspace(-1,1,100);\n",
    "y_p = np.polynomial.polynomial.polyval(x_p, coefs);\n",
    "sns.lineplot(x=x_p,y=y_p, color='red', label='True function');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use “transformed” features to fit a linear regression to this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_d3 = np.column_stack( [x_train**d for d in np.arange(1,4)])\n",
    "print(x_train_d3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_poly3 = LinearRegression().fit(x_train_d3,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reg_poly3.intercept_)\n",
    "print(reg_poly3.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s pretty good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=x_test.squeeze(), y=y_test, label='Test points');\n",
    "plt.xlabel(\"x\");\n",
    "plt.ylabel(\"y\");\n",
    "plt.title(\"Data generated from $1 + 0.5x + 2x^3$\");\n",
    "\n",
    "# Plot true function\n",
    "x_p = np.linspace(-1,1,100);\n",
    "y_p = np.polynomial.polynomial.polyval(x_p, coefs);\n",
    "sns.lineplot(x=x_p,y=y_p, color='red', label='True function');\n",
    "\n",
    "# Plot fitted function\n",
    "x_p = np.linspace(-1,1,100);\n",
    "x_p_d3 = np.column_stack( [x_p.reshape(-1,1)**d for d in np.arange(1,4)])\n",
    "y_p = reg_poly3.predict(x_p_d3);\n",
    "sns.lineplot(x=x_p, y=y_p, color='green', label='Model with d=3');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the error on the training set and on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_poly3_train = metrics.mean_squared_error(y_train, reg_poly3.predict(x_train_d3))\n",
    "mse_poly3_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_d3 = np.column_stack( [x_test**d for d in np.arange(1,4)])\n",
    "mse_poly3 = metrics.mean_squared_error(y_test, reg_poly3.predict(x_test_d3))\n",
    "mse_poly3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, though, we don’t know the “true” model order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we try fitting a linear regression (using transformed features) to this model, starting with $d=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_poly1 = LinearRegression().fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=x_test.squeeze(), y=y_test, label='Test points');\n",
    "plt.xlabel(\"x\");\n",
    "plt.ylabel(\"y\");\n",
    "plt.title(\"Data generated from $1 + 0.5x + 2x^3$\");\n",
    "\n",
    "# Plot true function\n",
    "x_p = np.linspace(-1,1,100);\n",
    "y_p = np.polynomial.polynomial.polyval(x_p, coefs);\n",
    "sns.lineplot(x=x_p,y=y_p, color='red', label='True function');\n",
    "\n",
    "# Plot fitted function\n",
    "x_p = np.linspace(-1,1,100);\n",
    "y_p = reg_poly1.predict(x_p.reshape(-1,1));\n",
    "sns.lineplot(x=x_p, y=y_p, color='green', label='Model with d=1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of *under-fitting* or *under-modeling*. The estimated function is not able to capture the complexity of the relation between $x$ and $y$ - it is not flexible enough.\n",
    "\n",
    "We can compute the error of this model, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_poly1_train = metrics.mean_squared_error(y_train, reg_poly1.predict(x_train))\n",
    "mse_poly1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_poly1 = metrics.mean_squared_error(y_test, reg_poly1.predict(x_test))\n",
    "mse_poly1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has higher error than the model with $d=3$, which is what we expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that we tried a model order that was too high, say $d=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_d10 = np.column_stack( [x_train**d for d in np.arange(1,11)])\n",
    "print(x_train_d10.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_poly10 = LinearRegression().fit(x_train_d10,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=x_test.squeeze(), y=y_test, label='Test points');\n",
    "plt.xlabel(\"x\");\n",
    "plt.ylabel(\"y\");\n",
    "plt.title(\"Data generated from $1 + 0.5x + 2x^3$\");\n",
    "\n",
    "# Plot true function\n",
    "x_p = np.linspace(-1,1,100);\n",
    "y_p = np.polynomial.polynomial.polyval(x_p, coefs);\n",
    "sns.lineplot(x=x_p,y=y_p, color='red', label='True function');\n",
    "\n",
    "# Plot fitted function\n",
    "x_p = np.linspace(-1,1,100)\n",
    "x_p_d10 = np.column_stack( [x_p.reshape(-1,1)**d for d in np.arange(1,11)]) \n",
    "y_p = reg_poly10.predict(x_p_d10);\n",
    "sns.lineplot(x=x_p, y=y_p, color='green', label='Model with d=10');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test error of this model is higher than the “true” model order, too. But, the training error is smaller!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_poly10_train = metrics.mean_squared_error(y_train, reg_poly10.predict(x_train_d10))\n",
    "mse_poly10_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_d10 = np.column_stack( [x_test**d for d in np.arange(1,11)])\n",
    "mse_poly10 = metrics.mean_squared_error(y_test, reg_poly10.predict(x_test_d10))\n",
    "mse_poly10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is called *over-fitting* or *over-modeling*. Because the model is very flexible, it is fitting the noise in the data and not the underlying relationship $y=t(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we compare the estimated function with the true function we can see the overfitting and underfitting clearly. But, in a real problem, we would not have access to the true function (otherwise, we wouldn’t need to be estimating it). The question then is if we can determine the correct model order from data.\n",
    "\n",
    "This problem is known as the *model order selection* problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One (bad) idea is for each model order to measure the MSE on the training data and select $d$ that minimizes the MSE. To do this, the code below loops over a model order `d = 1,2,...,14` and for each model order, fits a model and measures the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest_list = np.arange(1,20)\n",
    "mse_tr = np.zeros(len(dtest_list))\n",
    "mse_test = np.zeros(len(dtest_list))\n",
    "\n",
    "\n",
    "for didx, dtest in enumerate(dtest_list):\n",
    "\n",
    "    # get transformed features\n",
    "    x_train_dtest = np.column_stack( [x_train**d for d in np.arange(1,(dtest+1))])\n",
    "    x_test_dtest = np.column_stack( [x_test**d for d in np.arange(1,(dtest+1))])\n",
    "\n",
    "    # fit data\n",
    "    reg_dtest = LinearRegression().fit(x_train_dtest,y_train)\n",
    "    \n",
    "    # measure MSE on training data\n",
    "    y_hat = reg_dtest.predict(x_train_dtest)\n",
    "    mse_tr[didx] = metrics.mean_squared_error(y_train, y_hat)\n",
    "\n",
    "    # measure MSE on test data\n",
    "    y_hat_test = reg_dtest.predict(x_test_dtest)\n",
    "    mse_test[didx] = metrics.mean_squared_error(y_test, y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=dtest_list,y=mse_tr, marker=\"o\", label=\"Training\");\n",
    "sns.lineplot(x=dtest_list,y=mse_test, marker=\"o\", label=\"Test\");\n",
    "plt.xlabel('Model order');\n",
    "plt.ylabel('MSE');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increase $d$, the training MSE always decreases. So minimizing MSE on the training data leads to selecting a very high $d$ which in turn results in over-fitting.\n",
    "\n",
    "On the test data, we observe that the test MSE is high at first (due to high bias), then decreases, then increases again (due to high variance). The model with the smallest MSE on this particular test set may or may not be the model that best reflects the true relationship. (The performance on this test set is still subject to the random draw of samples in the test set!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting and underfitting: uninformative features\n",
    "----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting and underfitting can occur even with a “regular” linear regression on the original features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous lesson, we introduced a Python function that generates data using\n",
    "\n",
    "$$y_i = w_0 + w_1 x_{i,1} + \\ldots + w_d x_{i,d} + \\epsilon_i $$\n",
    "\n",
    "where $\\epsilon_i \\sim N(0, \\sigma^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will introduce a slight variation on this function: we will add the ability to include *uninformative* features. These features are not related to the target variable $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linear_regression_data(n=100, d=1, coef=[5], intercept=1, sigma=0, uninformative=0):\n",
    "  x = np.random.uniform(-1,1,size=(n,d-uninformative))\n",
    "  y = (np.dot(x, coef) + intercept).squeeze() + sigma * np.random.randn(n)\n",
    "  if uninformative:\n",
    "    x = np.column_stack((x, np.random.randn(n,uninformative)))\n",
    "\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let’s generate some data with 2 informative features (with some noise), and 2 uninformative features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = generate_linear_regression_data(n=100, d=4, coef=[5,5], intercept=1, sigma=1, uninformative=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plt.figure(figsize=(10,10));\n",
    "for i in range(4):\n",
    "  p = plt.subplot(2,2,i+1);\n",
    "  p = plt.scatter(x[:,i],  y);\n",
    "  p = plt.xlabel(\"x\" + str(i+1));\n",
    "  p = plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we fit a linear regression model, it is apparent that $x_3$ and $x_4$ do not affect $y$ - the coefficients $w_3$ and $w_4$ are close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_demo = LinearRegression().fit(x, y)\n",
    "print(\"Coefficient list: \", reg_demo.coef_)\n",
    "print(\"Intercept: \" , reg_demo.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we have many more uninformative features, though, and lots of noise relative to the true relationship?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "d = 95\n",
    "informative = 20\n",
    "coef = np.repeat(1,informative)\n",
    "sigma = 1\n",
    "intercept = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = generate_linear_regression_data(n=n_samples, d=d, coef=coef, intercept=intercept, sigma=sigma, uninformative=d-informative)\n",
    "x_test, y_test = generate_linear_regression_data(n=n_samples, d=d, coef=coef, intercept=intercept, sigma=sigma, uninformative=d-informative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_all = LinearRegression().fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows the coefficient value for each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.stem(np.arange(0, d),reg_all.coef_, bottom=0, use_line_collection=True);\n",
    "plt.xticks(np.arange(0, d, 1.0), rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the informative features don’t necessarily have coefficients with the largest magnitudes. The random “relationship” between an uninformative feature $x$ and $y$ appears, in some cases, as strong or stronger than the true relationship between an informative $x$ and $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compute R2 on the training set, everything looks great:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = reg_all.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(y_train, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R2 is great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what happens if we try to use our model to predict $y$ for some new, unseen data, from the same distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = reg_all.predict(x_test)\n",
    "metrics.r2_score(y_test, y_test_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s not good at all! Due to severe *over-fitting*, the model performs much worse on the test set than it did on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that, to avoid overfitting, we decide to use only a subset of features. We don’t know a priori which features are the most informative. This problem is known as the *feature selection problem*.\n",
    "\n",
    "Perhaps we may decide to try adding one feature at a time to our model, in order, and measure the MSE.\n",
    "\n",
    "(Obviously, there are better ways to do feature selection! But for our demo, this will suffice.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest_list = np.arange(1,d)\n",
    "r2_tr = np.zeros(len(dtest_list))\n",
    "r2_test = np.zeros(len(dtest_list))\n",
    "\n",
    "column_order = np.random.permutation(x_train.shape[1])\n",
    "x_train_shuffle = x_train[:, column_order]\n",
    "x_test_shuffle  = x_test[:, column_order]\n",
    "\n",
    "\n",
    "for didx, dtest in enumerate(dtest_list):\n",
    "\n",
    "    # get data with the right number of features\n",
    "    x_train_dtest = x_train_shuffle[:, :dtest]\n",
    "    x_test_dtest = x_test_shuffle[:, :dtest]\n",
    "\n",
    "    # fit data\n",
    "    reg_dtest = LinearRegression().fit(x_train_dtest,y_train)\n",
    "    \n",
    "    # measure R2 on training data\n",
    "    y_hat = reg_dtest.predict(x_train_dtest)\n",
    "    r2_tr[didx] = metrics.r2_score(y_train, y_hat)\n",
    "\n",
    "    # measure R2 on test data\n",
    "    y_hat_test = reg_dtest.predict(x_test_dtest)\n",
    "    r2_test[didx] = metrics.r2_score(y_test, y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=dtest_list,y=r2_tr, label=\"Training\");\n",
    "sns.lineplot(x=dtest_list,y=r2_test, label=\"Test\");\n",
    "plt.xlabel('Model order');\n",
    "plt.ylabel('R2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous example, where the model performed better on the training set with higher $d$, we see again that adding complexity reduces the training error and increases R2 on the training data - but not on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias and variance\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we are trying to estimate some value $y$, which is related to $x$ by some “true” function, $t(x)$.\n",
    "\n",
    "In general, $t(x)$ is not known to us. We do, however, have some “noisy” samples of $y$, generated from\n",
    "\n",
    "$$y = t(x) + \\epsilon$$\n",
    "\n",
    "where $\\epsilon \\sim N(0, \\sigma^2)$.\n",
    "\n",
    "We use these training samples to “learn” a function $f(x,w)$ parameterized by $w$ such that for samples in the training set,\n",
    "\n",
    "$$y \\approx f(x,w)$$\n",
    "\n",
    "Our ultimate goal, however, is to learn the function that will best approximate samples that we have not yet seen, but that have also been generated from\n",
    "\n",
    "$$y = t(x) + \\epsilon$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What might go wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias\n",
    "\n",
    "We might try to learn a class of function that is simply not capable of expressing $t(x)$.\n",
    "\n",
    "We observed this in the polynomial example, when we tried learning\n",
    "\n",
    "$$t(x) = w_0 + w_1 x + w_2 x^2 + w_3 x^3$$\n",
    "\n",
    "using\n",
    "\n",
    "$$f(x,w) = w_0 + w_1 x$$\n",
    "\n",
    "(or any $d < 3$.)\n",
    "\n",
    "We also observed this in the example with many features, when we tried learning\n",
    "\n",
    "$$t(x) = w_0 + w_1 x_1 + \\ldots + w_{20} x_{20}$$\n",
    "\n",
    "using a smaller number of features, e.g.\n",
    "\n",
    "$$f(x,w) = w_0 + w_1 x_1 + \\ldots + w_{10} x_{10}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance\n",
    "\n",
    "We might learn the right class of function, but due to the stochastic noise in the training samples, and the random draw of training samples, our parameter estimates are different from the “true” values.\n",
    "\n",
    "We observed this in the polynomial example, when we tried learning\n",
    "\n",
    "$$t(x) = w_0 + w_1 x + w_2 x^2 + w_3 x^3$$\n",
    "\n",
    "using\n",
    "\n",
    "$$f(x,w) = w_0 + w_1 x + \\ldots + w_d x^d$$\n",
    "\n",
    "with $d \\geq 3$. The more complex polynomial *is* capable of expressing $t(x)$ - if we would learn exactly the correct coefficients for $w_i, 0 \\leq i \\leq 3$ and $w_i = 0$ for $i > 3$. But, we had some error in the coefficient estimates, because our coefficient estimates are tuned to the noise in the training data. The more coefficients we learn, the more potential for error.\n",
    "\n",
    "We also observed this in the example with many features, when we tried learning\n",
    "\n",
    "$$t(x) = w_0 + w_1 x_1 + \\ldots + w_{20} x_{20}$$\n",
    "\n",
    "using\n",
    "\n",
    "$$f(x,w) = w_0 + w_1 x_1 + \\ldots + w_d x_d$$\n",
    "\n",
    "with $d \\geq 20$. Again, the model with extra uninformative features is *capable* of expressing $t(x)$ - if we would learn exactly the correct coefficients for $w_i, 0 \\leq i \\leq 20$ and $w_i = 0$ for $i > 20$. But, we had some error in the coefficient estimates, because our coefficient estimates are tuned to the noise in the training data. The more coefficients we learn, the more potential for error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Irreducible error\n",
    "\n",
    "Even if we learn exactly the right function, $f(x,w) = t(x)$, and we also learn the correct parameters, but still have error in our estimate of $y$ because of the stochastic “noise” in the data. We cannot “learn” $\\epsilon$, it is unpredictable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition of test MSE\n",
    "\n",
    "We know that the expected MSE on a new, unseen test point, can be decomposed into those three factors. Let us denote $E[f(x,w)]$ as $\\bar{f}(x,w)$, then\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Test MSE} &= E [(y - \\hat{y})^2] \\\\\n",
    "&= E [(t(x) + \\epsilon - f(x,w))^2] \\\\\n",
    "&= E (\\epsilon)^2 + E [(t(x) - f(x,w))^2] \\\\\n",
    "&=  E (\\epsilon)^2 + E[t(x) - \\bar{f}(x,w)]^2 + E[(f(x,w) - \\bar{f}(x,w))^2] \\\\\n",
    "&= \\sigma^2 + \\text{Bias}^2 + \\text{Var}(f(x,w))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where the expectation is over the draw of the training set and the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   The $\\sigma^2$ term captures the random noise in the data, which cannot be “learned”\n",
    "-   The $E[t(x) - \\bar{f}(x,w)]^2$ term captures any systematic difference between the function learned by our model and the true function, as will occur when the model cannot express the true function.\n",
    "-   The $E[(f(x,w) - \\bar{f}(x,w))^2]$ term captures the difference between the coefficient estimate due to a particular random training set, and the average coefficient estimate - in other words, the variance of the prediction over many fitted instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias and variance for linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall state here the following results, without proof, for a linear regression trained using least squares, with $n$ samples and having $d$ parameters. We also assume that $n \\geq d$ and the data matrix is full rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bias**: If there is no under-modeling, then on average, the estimate of $y$ is unbiased, i.e.\n",
    "\n",
    "$$\\text{Bias}(f(x,w)) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variance**: On average, the variance increases linearly with the number of parameters $d$ and inversely with the number of samples used for training $n$:\n",
    "\n",
    "$$\\text{Var}(f(x,w)) = \\frac{d}{n} \\sigma^2 $$\n",
    "\n",
    "where \\$ \\\\sigma^2\\$ is the stochastic noise variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your understanding with simulation\n",
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following pairs of regression models, answer:\n",
    "\n",
    "-   Which model in each pair will have greater expected variance of its estimate of $y$?\n",
    "-   Which model in each pair will have greater bias?\n",
    "\n",
    "(Assume that the two models in each pair are identical except for the differences that are specified.)\n",
    "\n",
    "Then, we’ll check our answer with a simulation experiment, in which we:\n",
    "\n",
    "-   Draw a test set from the distribution of the data\n",
    "-   Many times:\n",
    "    -   Draw a training set from the distribution of the data\n",
    "    -   Fit each model on the training set\n",
    "    -   Compute error on the test set\n",
    "\n",
    "**Note**: we can’t do this when applying machine learning to a real problem with real data! Why not?\n",
    "\n",
    "**Note**: this procedure won’t give us the *exact* bias and variance of the model that we could compute using the closed form expression - why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant model vs mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that data is generated as\n",
    "\n",
    "$$y = 1 + \\epsilon$$\n",
    "\n",
    "where $\\epsilon \\sim N(0, \\sigma^2)$.\n",
    "\n",
    "We will consider two models:\n",
    "\n",
    "**Model A** is a constant model:\n",
    "\n",
    "$$\\hat{y} = 1$$\n",
    "\n",
    "**Model B** is the mean of the training samples.\n",
    "\n",
    "$$\\hat{y} = \\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repeat = 1000\n",
    "n_test = 50\n",
    "n_train = 5\n",
    "sigma = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate test data once\n",
    "x_test = np.random.uniform(-1, 1, size=(n_test, 1))\n",
    "y_test = np.ones(n_test) + sigma*np.random.randn(n_test)\n",
    "y_test_no_noise = np.ones(n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare some matrices for storing simulation results\n",
    "y_predict = np.zeros((n_test, n_repeat, 2))\n",
    "\n",
    "y_test_err = np.zeros((n_test, n_repeat, 2))\n",
    "y_test_bias = np.zeros((n_test, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now simulate training the model many times, on different training data every time\n",
    "# and evaluate using the test data\n",
    "for i in range(n_repeat):\n",
    "\n",
    "  # train both models on newly generated training data\n",
    "  # note: we don't need X data since y is independent of X\n",
    "  y_train = np.ones(n_train) + sigma*np.random.randn(n_train)\n",
    "\n",
    "  # model A: predict constant always\n",
    "  y_predict[:, i, 0] = np.repeat(1, n_test) \n",
    "  # model B: predict mean of training samples\n",
    "  y_predict[:, i, 1] = np.mean(y_train)\n",
    "\n",
    "  # overall squared error is due to difference between f_t(x) + epsilon, model prediction f(x,w) \n",
    "  y_test_err[:, i, 0] = (y_test - y_predict[:, i, 0])**2\n",
    "  y_test_err[:, i, 1] = (y_test - y_predict[:, i, 1])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias is due to difference between f_t(x) and mean of model prediction f(x,w) \n",
    "y_test_bias[:, 0] = (y_test_no_noise - y_predict[:, :, 0].mean(axis=1))**2\n",
    "y_test_bias[:, 1] = (y_test_no_noise - y_predict[:, :, 1].mean(axis=1))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# irreducible error is due to the difference between f_t(x) and f_t(x) + epsilon\n",
    "y_test_noise_var = (y_test_no_noise - y_test)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plt.figure(figsize=(10,10));\n",
    "\n",
    "models = ['Model A', 'Model B']\n",
    "\n",
    "for midx, model in enumerate(models):\n",
    "  p = plt.subplot(2, 2, midx+1);\n",
    "  for i in range(n_repeat):\n",
    "    p = sns.lineplot(x=x_test.squeeze(), y=y_predict[:, i, midx], alpha=0.05, color='orange');\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_predict[:, :, midx].mean(axis=1), color='red', label='Mean of models');\n",
    "  p = sns.scatterplot(x=x_test[0:50].squeeze(), y=y_test[0:50], label='Test data');\n",
    "  p = plt.xlabel('x');\n",
    "  p = plt.ylabel('y');\n",
    "  p = plt.title(model);\n",
    "  p = plt.ylim(y_test.min()*1.1, y_test.max()*1.1)\n",
    " \n",
    "  p = plt.subplot(2, 2, midx+3);\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_test_err[:, :, midx].mean(axis=1), color='red', alpha=0.5, label='Total squared error', markers=True);\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_test_bias[:, midx], color='cyan', alpha=0.5, label='Bias^2');\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_predict[:, :, midx].var(axis=1), color='green', alpha=0.5, label='Variance');\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_test_noise_var, color='purple', alpha=0.2, label='Irreducible error');\n",
    "  p = plt.xlabel('x');\n",
    "  p = plt.ylabel('Error');\n",
    "  p = plt.title(\"MSE: %f \\n Irreducible error: %f \\n Bias^2: %f \\n Variance: %f \" % \n",
    "                (y_test_err[:,:,midx].mean(axis=(0,1)), \n",
    "                 y_test_noise_var.mean(), \n",
    "                 (y_test_bias[:, midx]).mean(),\n",
    "                  y_predict[:, :, midx].var(axis=1).mean() ) )\n",
    "  p = plt.ylim(0, 1.1* y_test_err[:, :, :].mean(axis=(1)).max() )\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question:\n",
    "\n",
    "-   Are the specific values of bias and variance consistent with our expectations?\n",
    "-   What would happen if \\$\\\\sigma = 0 \\$?\n",
    "-   What would happen if we used $\\hat{y} = 2$ for the constant model?\n",
    "-   What would happen if we would use $\\hat{y} = \\bar{y} + 2$ instead of $\\bar{y}$ for the mean model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean vs. linear model with d=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that data is generated as\n",
    "\n",
    "$$y = 1 + \\epsilon$$\n",
    "\n",
    "where $\\epsilon \\sim N(0, \\sigma^2)$.\n",
    "\n",
    "We will consider two models:\n",
    "\n",
    "**Model A** is a linear model with $d=1$ (using an uninformative feature, since $y$ is not a function of any $x$), trained using least squares:\n",
    "\n",
    "$$\\hat{y} = w_0 + w_1 x$$\n",
    "\n",
    "**Model B** is the mean of the training samples.\n",
    "\n",
    "$$\\hat{y} = \\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repeat = 1000\n",
    "n_test = 50\n",
    "n_train = 5\n",
    "sigma = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate test data once\n",
    "x_test = np.random.uniform(-1, 1, size=(n_test, 1))\n",
    "y_test = np.ones(n_test) + sigma*np.random.randn(n_test)\n",
    "y_test_no_noise = np.ones(n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare some matrices for storing simulation results\n",
    "y_predict = np.zeros((n_test, n_repeat, 2))\n",
    "\n",
    "y_test_err = np.zeros((n_test, n_repeat, 2))\n",
    "y_test_bias = np.zeros((n_test, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now simulate training the model many times, on different training data every time\n",
    "# and evaluate using the test data\n",
    "for i in range(n_repeat):\n",
    "\n",
    "  # train both models on newly generated training data\n",
    "  y_train = np.ones(n_train) + sigma*np.random.randn(n_train)\n",
    "  x_train = np.random.uniform(-1,1, size=(n_train, 1))\n",
    "\n",
    "  # model A: fit linear model\n",
    "  reg_a = LinearRegression().fit(x_train, y_train)\n",
    "  y_predict[:, i, 0] = reg_a.predict(x_test)\n",
    "  # model B: predict mean of training samples\n",
    "  y_predict[:, i, 1] = np.mean(y_train)\n",
    "\n",
    "  # overall squared error is due to difference between f_t(x) + epsilon, model prediction f(x,w) \n",
    "  y_test_err[:, i, 0] = (y_test - y_predict[:, i, 0])**2\n",
    "  y_test_err[:, i, 1] = (y_test - y_predict[:, i, 1])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias is due to difference between f_t(x) and mean of model prediction f(x,w) \n",
    "y_test_bias[:, 0] = (y_test_no_noise - y_predict[:, :, 0].mean(axis=1))**2\n",
    "y_test_bias[:, 1] = (y_test_no_noise - y_predict[:, :, 1].mean(axis=1))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# irreducible error is due to the difference between f_t(x) and f_t(x) + epsilon\n",
    "y_test_noise_var = (y_test_no_noise - y_test)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plt.figure(figsize=(10,10));\n",
    "\n",
    "models = ['Model A', 'Model B']\n",
    "\n",
    "for midx, model in enumerate(models):\n",
    "  p = plt.subplot(2, 2, midx+1);\n",
    "  for i in range(n_repeat):\n",
    "    p = sns.lineplot(x=x_test.squeeze(), y=y_predict[:, i, midx], alpha=0.05, color='orange');\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_predict[:, :, midx].mean(axis=1), color='red', label='Mean of models');\n",
    "  p = sns.scatterplot(x=x_test[0:50].squeeze(), y=y_test[0:50], label='Test data');\n",
    "  p = plt.xlabel('x');\n",
    "  p = plt.ylabel('y');\n",
    "  p = plt.title(model);\n",
    "  p = plt.ylim(y_test.min()*1.1, y_test.max()*1.1)\n",
    "\n",
    "  p = plt.subplot(2, 2, midx+3);\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_test_err[:, :, midx].mean(axis=1), color='red', alpha=0.5, label='Total squared error', markers=True);\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_test_bias[:, midx], color='cyan', alpha=0.5, label='Bias^2');\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_predict[:, :, midx].var(axis=1), color='green', alpha=0.5, label='Variance');\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_test_noise_var, color='purple', alpha=0.2, label='Irreducible error');\n",
    "  p = plt.xlabel('x');\n",
    "  p = plt.ylabel('Error');\n",
    "  p = plt.title(\"MSE: %f \\n Irreducible error: %f \\n Bias^2: %f \\n Variance: %f \" % \n",
    "                (y_test_err[:,:,midx].mean(axis=(0,1)), \n",
    "                 y_test_noise_var.mean(), \n",
    "                 (y_test_bias[:, midx]).mean(),\n",
    "                  y_predict[:, :, midx].var(axis=1).mean() ) )\n",
    "  p = plt.ylim(0, 1.1* y_test_err[:, :, :].mean(axis=(1)).max() )\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model with 10 vs. 1000 training samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model A**:\n",
    "\n",
    "$$\\hat{y} = w_0 + w_1 x_1$$\n",
    "\n",
    "with least squares estimate of parameters, $n = 10$ training samples.\n",
    "\n",
    "**Model B**:\n",
    "\n",
    "$$\\hat{y} = w_0 + w_1 x_1$$\n",
    "\n",
    "with least squares estimate of parameters, $n = 1000$ training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repeat = 1000\n",
    "n_test = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.zeros((n_test, n_repeat, 2))\n",
    "\n",
    "y_test_err = np.zeros((n_test, n_repeat, 2))\n",
    "y_test_bias = np.zeros((n_test, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = [5]\n",
    "intercept = 1\n",
    "sigma = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate test data once\n",
    "x_test, y_test = generate_linear_regression_data(n=n_test, d=1, coef=[5], intercept=1, sigma=sigma)\n",
    "y_test_no_noise = intercept + np.dot(x_test, coef) \n",
    "# note: y_test is f_t(x) + epsilon\n",
    "# note: y_test_no_noise is f_t(x) \n",
    "\n",
    "# noise is due to the difference between f_t(x) and f_t(x) + epsilon\n",
    "y_test_noise_var = (y_test_no_noise - y_test)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now simulate training the model many times, on different training data every time\n",
    "# and evaluate using the test data\n",
    "for i in range(n_repeat):\n",
    "\n",
    "  # train both models on newly generated training data\n",
    "  x, y = generate_linear_regression_data(n=1000, d=1, coef=coef, intercept=intercept, sigma=sigma)\n",
    "  x_a, y_a = x[0:10], y[0:10]\n",
    "  reg_a = LinearRegression().fit(x_a, y_a)\n",
    "\n",
    "  x_b, y_b = x, y\n",
    "  reg_b = LinearRegression().fit(x_b, y_b)\n",
    "\n",
    "  y_predict[:, i, 0] = reg_a.predict(x_test)\n",
    "  y_predict[:, i, 1] = reg_b.predict(x_test)\n",
    "\n",
    "  # overall squared error is due to difference between f_t(x) + epsilon, model prediction f(x,w) \n",
    "  y_test_err[:, i, 0] = (y_test - y_predict[:, i, 0])**2\n",
    "  y_test_err[:, i, 1] = (y_test - y_predict[:, i, 1])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias is due to difference between f_t(x) and mean of model prediction f(x,w) \n",
    "y_test_bias[:, 0] = (y_test_no_noise - y_predict[:, :, 0].mean(axis=1))**2\n",
    "y_test_bias[:, 1] = (y_test_no_noise - y_predict[:, :, 1].mean(axis=1))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plt.figure(figsize=(10,10));\n",
    "\n",
    "models = ['Model A', 'Model B']\n",
    "\n",
    "for midx, model in enumerate(models):\n",
    "  p = plt.subplot(2, 2, midx+1);\n",
    "  for i in range(n_repeat):\n",
    "    p = sns.lineplot(x=x_test.squeeze(), y=y_predict[:, i, midx], alpha=0.05, color='orange');\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_predict[:, :, midx].mean(axis=1), color='red', label='Mean of models');\n",
    "  p = sns.scatterplot(x=x_test.squeeze(), y=y_test, label='Test data');\n",
    "  p = plt.xlabel('x');\n",
    "  p = plt.ylabel('y');\n",
    "  p = plt.title(model);\n",
    "  p = plt.ylim(y_test.min()*1.1, y_test.max()*1.1)\n",
    "\n",
    "  p = plt.subplot(2, 2, midx+3);\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_test_err[:, :, midx].mean(axis=1), color='red', alpha=0.5, label='Total squared error', markers=True);\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_test_bias[:, midx], color='cyan', alpha=0.5, label='Bias^2');\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_predict[:, :, midx].var(axis=1), color='green', alpha=0.5, label='Variance');\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_test_noise_var, color='purple', alpha=0.2, label='Irreducible error');\n",
    "  p = plt.xlabel('x');\n",
    "  p = plt.ylabel('Error');\n",
    "  p = plt.title(\"MSE: %f \\n Irreducible error: %f \\n Bias^2: %f \\n Variance: %f \" % \n",
    "                (y_test_err[:,:,midx].mean(axis=(0,1)), \n",
    "                 y_test_noise_var.mean(), \n",
    "                 (y_test_bias[:, midx]).mean(),\n",
    "                  y_predict[:, :, midx].var(axis=1).mean() ) )\n",
    "  p = plt.ylim(0, 1.1* y_test_err[:, :, :].mean(axis=(1)).max() )\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear vs. polynomial model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is generated as $y_i = 1 + 0.5x_i + 2x_i^3 + \\epsilon_i$.\n",
    "\n",
    "**Model A**:\n",
    "\n",
    "$$\\hat{y} = w_0 + w_1 x$$\n",
    "\n",
    "with least squares estimate of parameters.\n",
    "\n",
    "**Model B**:\n",
    "\n",
    "$$\\hat{y} = w_0 + w_1 x + w_2 x^2 + w_3 x^3$$\n",
    "\n",
    "with least squares estimate of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repeat = 1000\n",
    "n_test = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.zeros((n_test, n_repeat, 2))\n",
    "\n",
    "y_test_err = np.zeros((n_test, n_repeat, 2))\n",
    "y_test_bias = np.zeros((n_test, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs=[1,0.5,0,2]\n",
    "sigma = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate test data once\n",
    "x_test, y_test = generate_polynomial_regression_data(n=n_test, coefs=coefs, sigma=sigma)\n",
    "y_test_no_noise = np.polynomial.polynomial.polyval(x_test, coefs).squeeze()\n",
    "# note: y_test is f_t(x) + epsilon\n",
    "# note: y_test_no_noise is f_t(x) \n",
    "\n",
    "# noise is due to the difference between f_t(x) and f_t(x) + epsilon\n",
    "y_test_noise_var = (y_test_no_noise - y_test)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now simulate training the model many times, on different training data every time\n",
    "# and evaluate using the test data\n",
    "for i in range(n_repeat):\n",
    "\n",
    "  # train both models on newly generated training data\n",
    "  x, y = generate_polynomial_regression_data(n=100, coefs=coefs, sigma=sigma)\n",
    "\n",
    "  x_a, y_a = x, y\n",
    "  reg_a = LinearRegression().fit(x_a, y_a)\n",
    "\n",
    "  x_b_d3 = np.column_stack( [x.reshape(-1,1)**d for d in np.arange(1,4)])\n",
    "  y_b = y\n",
    "  reg_b = LinearRegression().fit(x_b_d3, y_b)\n",
    "\n",
    "  y_predict[:, i, 0] = reg_a.predict(x_test)\n",
    "  x_test_d3 = np.column_stack( [x_test.reshape(-1,1)**d for d in np.arange(1,4)])\n",
    "  y_predict[:, i, 1] = reg_b.predict(x_test_d3)\n",
    "\n",
    "  # overall squared error is due to difference between f_t(x) + epsilon, model prediction f(x,w) \n",
    "  y_test_err[:, i, 0] = (y_test - y_predict[:, i, 0])**2\n",
    "  y_test_err[:, i, 1] = (y_test - y_predict[:, i, 1])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias is due to difference between f_t(x) and mean of model prediction f(x,w) \n",
    "y_test_bias[:, 0] = (y_test_no_noise - y_predict[:, :, 0].mean(axis=1))**2\n",
    "y_test_bias[:, 1] = (y_test_no_noise - y_predict[:, :, 1].mean(axis=1))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plt.figure(figsize=(10,10));\n",
    "\n",
    "models = ['Model A', 'Model B']\n",
    "\n",
    "for midx, model in enumerate(models):\n",
    "  p = plt.subplot(2, 2, midx+1);\n",
    "  for i in range(n_repeat):\n",
    "    p = sns.lineplot(x=x_test.squeeze(), y=y_predict[:, i, midx], alpha=0.05, color='orange');\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_predict[:, :, midx].mean(axis=1), color='red', label='Mean of models');\n",
    "  p = sns.scatterplot(x=x_test.squeeze(), y=y_test, label='Test data');\n",
    "  p = plt.xlabel('x');\n",
    "  p = plt.ylabel('y');\n",
    "  p = plt.title(model);\n",
    "  p = plt.ylim(y_test.min()*1.1, y_test.max()*1.1)\n",
    "\n",
    "  p = plt.subplot(2, 2, midx+3);\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_test_err[:, :, midx].mean(axis=1), color='red', alpha=0.5, label='Total squared error', markers=True);\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_test_bias[:, midx], color='cyan', alpha=0.5, label='Bias^2');\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_predict[:, :, midx].var(axis=1), color='green', alpha=0.5, label='Variance');\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_test_noise_var, color='purple', alpha=0.2, label='Irreducible error');\n",
    "  p = plt.xlabel('x');\n",
    "  p = plt.ylabel('Error');\n",
    "  p = plt.title(\"MSE: %f \\n Irreducible error: %f \\n Bias^2: %f \\n Variance: %f \" % \n",
    "                (y_test_err[:,:,midx].mean(axis=(0,1)), \n",
    "                 y_test_noise_var.mean(), \n",
    "                 (y_test_bias[:, midx]).mean(),\n",
    "                  y_predict[:, :, midx].var(axis=1).mean() ) )\n",
    "  p = plt.ylim(0, 1.1* y_test_err[:, :, :].mean(axis=(1)).max() )\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "-   What would happen if you increase or decrease $\\sigma$ in this example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Double descent: https://colab.research.google.com/github/aslanides/aslanides.github.io/blob/master/colabs/2019-10-10-interpolation-regime.ipynb#scrollTo=nlaWee2cN7K7 \n",
    "https://mlu-explain.github.io/double-descent/\n",
    "https://mlu-explain.github.io/double-descent2/\n",
    "-->"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
