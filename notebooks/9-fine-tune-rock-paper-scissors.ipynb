{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo: Transfer learning\n",
    "=======================\n",
    "\n",
    "*This is based on the course of [Fraida Fund](https://colab.research.google.com/github/ffund/ml-notebooks/blob/master/notebooks/1-colab-tour.ipynb) for  NYU Tandon School of Engineering*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, for most machine learning problems, you wouldn’t design or train a convolutional neural network from scratch - you would use an existing model that suits your needs (does well on ImageNet, size is right) and fine-tune it on your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: for faster training, use Runtime \\> Change Runtime Type to run this notebook on a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import platform\n",
    "import datetime\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "\n",
    "print('Python version:', platform.python_version())\n",
    "print('Tensorflow version:', tf.__version__)\n",
    "print('Keras version:', tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The “rock paper scissors” dataset is available directly from the Tensorflow package. In the cells that follow, we’ll get the data, plot a few examples, and also do some preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'rock_paper_scissors',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    with_info=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = tfds.show_examples(ds_info, ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array(['rock', 'paper', 'scissors'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process dataset\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_IMG_SIZE = 224\n",
    "INPUT_IMG_SHAPE = (224, 224, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(sample):\n",
    "    sample['image'] = tf.cast(sample['image'], tf.float32)\n",
    "    sample['image'] = sample['image'] / 255.\n",
    "    sample['image'] = tf.image.resize(sample['image'], [INPUT_IMG_SIZE, INPUT_IMG_SIZE])\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds_train.map(preprocess_image)\n",
    "ds_test  = ds_test.map(preprocess_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = tfds.show_examples(ds_train, ds_info, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’l convert to `numpy` format again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_numpy = np.vstack(tfds.as_numpy(ds_train))\n",
    "test_numpy = np.vstack(tfds.as_numpy(ds_test))\n",
    "\n",
    "X_train = np.array(list(map(lambda x: x[0]['image'], train_numpy)))\n",
    "y_train = np.array(list(map(lambda x: x[0]['label'], train_numpy)))\n",
    "\n",
    "X_test = np.array(list(map(lambda x: x[0]['image'], test_numpy)))\n",
    "y_test = np.array(list(map(lambda x: x[0]['label'], test_numpy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload custom test sample\n",
    "-------------------------\n",
    "\n",
    "This code expects a PNG image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    " \n",
    "# Edit the filename here as needed\n",
    "filename = 'scissors.png'\n",
    " \n",
    "# pre-process image\n",
    "image = Image.open(filename).convert('RGB')\n",
    "image_resized = image.resize((INPUT_IMG_SIZE, INPUT_IMG_SIZE), Image.BICUBIC)\n",
    "test_sample = np.array(image_resized)/255.0\n",
    "test_sample = test_sample.reshape(1, INPUT_IMG_SIZE, INPUT_IMG_SIZE, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(4,4));\n",
    "plt.imshow(test_sample.reshape(INPUT_IMG_SIZE, INPUT_IMG_SIZE, 3));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify with MobileNetV2\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Keras Applications](https://keras.io/api/applications/) are pre-trained models with saved weights, that you can download and use without any additional training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a table of the models available as Keras Applications.\n",
    "\n",
    "In this table, the top-1 and top-5 accuracy refer to the model's performance on the ImageNet validation dataset, and depth is the depth of the network including activation layers, batch normalization layers, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>Model</th>\n",
    "<th align=\"right\">Size</th>\n",
    "<th align=\"right\">Top-1 Accuracy</th>\n",
    "<th align=\"right\">Top-5 Accuracy</th>\n",
    "<th align=\"right\">Parameters</th>\n",
    "<th align=\"right\">Depth</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>Xception</td>\n",
    "<td align=\"right\">88 MB</td>\n",
    "<td align=\"right\">0.790</td>\n",
    "<td align=\"right\">0.945</td>\n",
    "<td align=\"right\">22,910,480</td>\n",
    "<td align=\"right\">126</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>VGG16</td>\n",
    "<td align=\"right\">528 MB</td>\n",
    "<td align=\"right\">0.713</td>\n",
    "<td align=\"right\">0.901</td>\n",
    "<td align=\"right\">138,357,544</td>\n",
    "<td align=\"right\">23</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>VGG19</td>\n",
    "<td align=\"right\">549 MB</td>\n",
    "<td align=\"right\">0.713</td>\n",
    "<td align=\"right\">0.900</td>\n",
    "<td align=\"right\">143,667,240</td>\n",
    "<td align=\"right\">26</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ResNet50</td>\n",
    "<td align=\"right\">98 MB</td>\n",
    "<td align=\"right\">0.749</td>\n",
    "<td align=\"right\">0.921</td>\n",
    "<td align=\"right\">25,636,712</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ResNet101</td>\n",
    "<td align=\"right\">171 MB</td>\n",
    "<td align=\"right\">0.764</td>\n",
    "<td align=\"right\">0.928</td>\n",
    "<td align=\"right\">44,707,176</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ResNet152</td>\n",
    "<td align=\"right\">232 MB</td>\n",
    "<td align=\"right\">0.766</td>\n",
    "<td align=\"right\">0.931</td>\n",
    "<td align=\"right\">60,419,944</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ResNet50V2</td>\n",
    "<td align=\"right\">98 MB</td>\n",
    "<td align=\"right\">0.760</td>\n",
    "<td align=\"right\">0.930</td>\n",
    "<td align=\"right\">25,613,800</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ResNet101V2</td>\n",
    "<td align=\"right\">171 MB</td>\n",
    "<td align=\"right\">0.772</td>\n",
    "<td align=\"right\">0.938</td>\n",
    "<td align=\"right\">44,675,560</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ResNet152V2</td>\n",
    "<td align=\"right\">232 MB</td>\n",
    "<td align=\"right\">0.780</td>\n",
    "<td align=\"right\">0.942</td>\n",
    "<td align=\"right\">60,380,648</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>InceptionV3</td>\n",
    "<td align=\"right\">92 MB</td>\n",
    "<td align=\"right\">0.779</td>\n",
    "<td align=\"right\">0.937</td>\n",
    "<td align=\"right\">23,851,784</td>\n",
    "<td align=\"right\">159</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>InceptionResNetV2</td>\n",
    "<td align=\"right\">215 MB</td>\n",
    "<td align=\"right\">0.803</td>\n",
    "<td align=\"right\">0.953</td>\n",
    "<td align=\"right\">55,873,736</td>\n",
    "<td align=\"right\">572</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MobileNet</td>\n",
    "<td align=\"right\">16 MB</td>\n",
    "<td align=\"right\">0.704</td>\n",
    "<td align=\"right\">0.895</td>\n",
    "<td align=\"right\">4,253,864</td>\n",
    "<td align=\"right\">88</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MobileNetV2</td>\n",
    "<td align=\"right\">14 MB</td>\n",
    "<td align=\"right\">0.713</td>\n",
    "<td align=\"right\">0.901</td>\n",
    "<td align=\"right\">3,538,984</td>\n",
    "<td align=\"right\">88</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>DenseNet121</td>\n",
    "<td align=\"right\">33 MB</td>\n",
    "<td align=\"right\">0.750</td>\n",
    "<td align=\"right\">0.923</td>\n",
    "<td align=\"right\">8,062,504</td>\n",
    "<td align=\"right\">121</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>DenseNet169</td>\n",
    "<td align=\"right\">57 MB</td>\n",
    "<td align=\"right\">0.762</td>\n",
    "<td align=\"right\">0.932</td>\n",
    "<td align=\"right\">14,307,880</td>\n",
    "<td align=\"right\">169</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>DenseNet201</td>\n",
    "<td align=\"right\">80 MB</td>\n",
    "<td align=\"right\">0.773</td>\n",
    "<td align=\"right\">0.936</td>\n",
    "<td align=\"right\">20,242,984</td>\n",
    "<td align=\"right\">201</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>NASNetMobile</td>\n",
    "<td align=\"right\">23 MB</td>\n",
    "<td align=\"right\">0.744</td>\n",
    "<td align=\"right\">0.919</td>\n",
    "<td align=\"right\">5,326,716</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>NASNetLarge</td>\n",
    "<td align=\"right\">343 MB</td>\n",
    "<td align=\"right\">0.825</td>\n",
    "<td align=\"right\">0.960</td>\n",
    "<td align=\"right\">88,949,818</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>EfficientNetB0</td>\n",
    "<td align=\"right\">29 MB</td>\n",
    "<td align=\"right\">-</td>\n",
    "<td align=\"right\">-</td>\n",
    "<td align=\"right\">5,330,571</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>EfficientNetB1</td>\n",
    "<td align=\"right\">31 MB</td>\n",
    "<td align=\"right\">-</td>\n",
    "<td align=\"right\">-</td>\n",
    "<td align=\"right\">7,856,239</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>EfficientNetB2></td>\n",
    "<td align=\"right\">36 MB</td>\n",
    "<td align=\"right\">-</td>\n",
    "<td align=\"right\">-</td>\n",
    "<td align=\"right\">9,177,569</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>EfficientNetB3</td>\n",
    "<td align=\"right\">48 MB</td>\n",
    "<td align=\"right\">-</td>\n",
    "<td align=\"right\">-</td>\n",
    "<td align=\"right\">12,320,535</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>EfficientNetB4</td>\n",
    "<td align=\"right\">75 MB</td>\n",
    "<td align=\"right\">-</td>\n",
    "<td align=\"right\">-</td>\n",
    "<td align=\"right\">19,466,823</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>EfficientNetB5</td>\n",
    "<td align=\"right\">118 MB</td>\n",
    "<td align=\"right\">-</td>\n",
    "<td align=\"right\">-</td>\n",
    "<td align=\"right\">30,562,527</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>EfficientNetB6</td>\n",
    "<td align=\"right\">166 MB</td>\n",
    "<td align=\"right\">-</td>\n",
    "<td align=\"right\">-</td>\n",
    "<td align=\"right\">43,265,143</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>EfficientNetB7</td>\n",
    "<td align=\"right\">256 MB</td>\n",
    "<td align=\"right\">-</td>\n",
    "<td align=\"right\">-</td>\n",
    "<td align=\"right\">66,658,687</td>\n",
    "<td align=\"right\">-</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(A variety of other models is available from other sources - for example, the [Tensorflow Hub](https://tfhub.dev/).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to use MobileNetV2, which is designed specifically to be small and fast (so it can run on mobile devices!)\n",
    "\n",
    "MobileNets come in various sizes controlled by a multiplier for the depth (number of features), and trained for various sizes of input images. We will use the 224x224 input image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "  input_shape=INPUT_IMG_SHAPE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_probs = base_model.predict(test_sample)\n",
    "base_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = tf.keras.utils.get_file(\n",
    "    'ImageNetLabels.txt',\n",
    "    'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\n",
    "imagenet_classes = np.array(open(url).read().splitlines())[1:]\n",
    "imagenet_classes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see what the top 5 predicted classes are for my test image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_likely_classes = np.argsort(base_probs.squeeze())[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4));\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(test_sample.reshape(INPUT_IMG_SIZE, INPUT_IMG_SIZE, 3));\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "p = sns.barplot(x=imagenet_classes[most_likely_classes],y=base_probs.squeeze()[most_likely_classes]);\n",
    "plt.ylabel(\"Probability\");\n",
    "p.set_xticklabels(p.get_xticklabels(), rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MobileNetV2 is trained on a specific task: classifying the images in the ImageNet dataset by selecting the most appropriate of 1000 class labels.\n",
    "\n",
    "It is not trained for our specific task: classifying an image of a hand as rock, paper, or scissors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Background: fine-tuning a model\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical convolutional neural network looks something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image via [PlotNeuralNet](https://github.com/HarisIqbal88/PlotNeuralNet)](https://raw.githubusercontent.com/LongerVision/Resource/master/AI/Visualization/PlotNeuralNet/vgg16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a sequence of convolutional layers followed by pooling layers. These layers are *feature extractors* that “learn” key features of our input images.\n",
    "\n",
    "Then, we have one or more fully connected layers followed by a fully connected layer with a softmax activation function. This part of the network is for *classification*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key idea behind transfer learning is that the *feature extractor* part of the network can be re-used across different tasks and different domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is especially useful when we don’t have a lot of task-specific data. We can get a pre-trained feature extractor trained on a lot of data from another task, then train the classifier on task-specific data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general process is:\n",
    "\n",
    "-   Get a pre-trained model, without the classification layer.\n",
    "-   Freeze the base model.\n",
    "-   Add a classification layer.\n",
    "-   Train the model (only the weights in your classification layer will be updated).\n",
    "-   (Optional) Un-freeze some of the last layers in your base model.\n",
    "-   (Optional) Train the model again, with a smaller learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train our own classification head\n",
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we will get the MobileNetV2 model *without* the fully connected layer at the top of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "  input_shape=INPUT_IMG_SHAPE,\n",
    "  include_top=False, \n",
    "  pooling='avg'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will *freeze* the model. We're not going to train the MobileNetV2 part of the model, we're just going to use it to extract features from the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’l make a *new* model out of the “headless” already-fitted MobileNetV2, with a brand-new, totally untrained classification head on top:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(base_model)\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(\n",
    "    units=3,\n",
    "    activation=tf.keras.activations.softmax\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’l compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we’ll use data augmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_gen = ImageDataGenerator(rotation_range=8, width_shift_range=0.08, shear_range=0.3,\n",
    "                         height_shift_range=0.08, zoom_range=0.08)\n",
    "train_generator = train_gen.flow(X_train, y_train, batch_size=BATCH_SIZE)\n",
    "\n",
    "val_gen = ImageDataGenerator()\n",
    "val_generator = val_gen.flow(X_test, y_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start training our model. Remember, we are *only* updating the weights in the classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "\n",
    "hist = model.fit(\n",
    "    train_generator, \n",
    "    epochs=n_epochs,\n",
    "    steps_per_epoch=X_train.shape[0]//BATCH_SIZE,\n",
    "    validation_data=val_generator, \n",
    "    validation_steps=X_test.shape[0]//BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = hist.history['loss']\n",
    "val_loss = hist.history['val_loss']\n",
    "\n",
    "accuracy = hist.history['accuracy']\n",
    "val_accuracy = hist.history['val_accuracy']\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(loss, label='Training set')\n",
    "plt.plot(val_loss, label='Test set', linestyle='--')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(accuracy, label='Training set')\n",
    "plt.plot(val_accuracy, label='Test set', linestyle='--')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune model\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have fitted our own classification head, but there's one more step we can attempt to customize the model for our particular application.\n",
    "\n",
    "We are going to “un-freeze” the later parts of the model, and train it for a few more epochs on our data, so that the high-level features are better suited for our specific classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(base_model.layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are *not* creating a new model. We're just going to continue training the model we already started training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_at = 149\n",
    "\n",
    "# freeze first layers\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable =  False\n",
    "    \n",
    "# use a smaller training rate for fine-tuning\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "model.compile(\n",
    "    optimizer = opt,\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs_fine = 20\n",
    "\n",
    "hist_fine = model.fit( \n",
    "    train_generator, \n",
    "    epochs=n_epochs + n_epochs_fine,\n",
    "    initial_epoch=n_epochs,  \n",
    "    steps_per_epoch=X_train.shape[0]//BATCH_SIZE,\n",
    "    validation_data=val_generator, \n",
    "    validation_steps=X_test.shape[0]//BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = hist.history['loss'] + hist_fine.history['loss']\n",
    "val_loss = hist.history['val_loss'] + hist_fine.history['val_loss']\n",
    "\n",
    "accuracy = hist.history['accuracy'] + hist_fine.history['accuracy']\n",
    "val_accuracy = hist.history['val_accuracy'] + hist_fine.history['val_accuracy']\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(loss, label='Training set')\n",
    "plt.plot(val_loss, label='Test set', linestyle='--')\n",
    "plt.plot([n_epochs, n_epochs], plt.ylim(),label='Fine Tuning',linestyle='dotted')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(accuracy, label='Training set')\n",
    "plt.plot(val_accuracy, label='Test set', linestyle='dotted')\n",
    "plt.plot([n_epochs, n_epochs], plt.ylim(), label='Fine Tuning', linestyle='--')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify custom test sample\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs = model.predict(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4));\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(test_sample.reshape(INPUT_IMG_SIZE, INPUT_IMG_SIZE, 3));\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "p = sns.barplot(x=classes,y=test_probs.squeeze());\n",
    "plt.ylabel(\"Probability\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some comments\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, for most machine learning problems, you wouldn’t design or train a convolutional neural network from scratch - you would use an existing model that suits your needs (does well on ImageNet, size is right) and fine-tune it on your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning isn’t only for image classification.\n",
    "\n",
    "There are many problems that can be solved by taking a VERY LARGE task-generic “feature detection” model trained on a LOT of data, and fine-tuning it on a small custom dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, consider [AI Dungeon](https://play.aidungeon.io/), a game in the style of classic text-based adventure games.\n",
    "\n",
    "It was trained by fine-tuning a version of GPT-2. GPT-2 is a language model with 1.5 billion parameters, trained on a dataset of 8 million web pages, with the objective of predicting the next word in a sequence.\n",
    "\n",
    "The creator of AI Dungeon fine-tuned GTP-2 using story-games scraped from [ChooseYourStory.com](https://chooseyourstory.com/Stories/).\n",
    "\n",
    "With so much data, the model doesn’t only learn about language and language features - it learns about the world described by all that text! Since the game is based on a fine-tuned version of that model, it also knows a lot about the world."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "colab": {
   "toc_visible": "true"
  }
 }
}
