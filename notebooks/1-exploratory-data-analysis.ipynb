{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1291841e",
   "metadata": {},
   "source": [
    "Exploratory data analysis\n",
    "=========================\n",
    "\n",
    "*This is based on the course of [Fraida Fund](https://colab.research.google.com/github/ffund/ml-notebooks/blob/master/notebooks/1-colab-tour.ipynb) for  NYU Tandon School of Engineering*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook\n",
    "----------------\n",
    "\n",
    "In this notebook:\n",
    "\n",
    "-   We practice using `pandas` to read in and manipulate a data set. (We\n",
    "    won’t have a separate tutorial on `pandas` - we will learn basic\n",
    "    `pandas` techniques as we need them.)\n",
    "-   We learn a basic “recipe” for exploratory data analysis and apply it\n",
    "    to an example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in applying machine learning to a real problem is\n",
    "*finding* or *creating* an appropriate data set with which to train your\n",
    "model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What makes data “good”?\n",
    "\n",
    "What makes a good data set?\n",
    "\n",
    "-   **Size**: the more *samples* are in the data set, the more examples\n",
    "    your machine learning model will be able to learn from, and the\n",
    "    better it will do. Often, a simple machine learning model trained on\n",
    "    a large data set will outperform a “fancy” model on a small data\n",
    "    set.\n",
    "-   **Quality**: Are there *predictive* features in the data? Are no\n",
    "    values (or very few values) missing, noisy, or incorrect? Is the\n",
    "    scenario in which the data collected similar to the scenario in\n",
    "    which your model will be used? These are examples of questions that\n",
    "    we might ask to evaluate the quality of a data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important principles in machine learning is: **garbage\n",
    "in, garbage out**. If the data you use to train a machine learning model\n",
    "is problematic, or not well suited for the purpose, then even the best\n",
    "model will produce useless predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose of exploratory data analysis\n",
    "\n",
    "Once we have identified one or more candidate data sets for a particular\n",
    "problem, we perform some *exploratory data analysis*. This process helps\n",
    "us\n",
    "\n",
    "-   detect and possibly correct mistakes in the data\n",
    "-   check our assumptions about the data\n",
    "-   identify potential relationships between features\n",
    "-   assess the direction and rough size of relationships between\n",
    "    features and the target variable\n",
    "\n",
    "Exploratory data analysis is important for understanding whether this\n",
    "data set is appropriate for the machine learning task at hand, and if\n",
    "any extra cleaning or processing steps are required before we use the\n",
    "data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“Recipe” for exploratory data analysis\n",
    "--------------------------------------\n",
    "\n",
    "We will practice using a basic “recipe” for exploratory data analysis.\n",
    "\n",
    "1.  Set down *expectations* about the data\n",
    "2.  Load data and check that it is loaded correctly\n",
    "3.  Inspect the data to make sure it is consistent with your\n",
    "    expectations (“sanity checks”), and clean or filter the data if\n",
    "    needed\n",
    "4.  Explore relationships in the data to identify good candidate\n",
    "    features and target variables\n",
    "\n",
    "Every exploratory data analysis is different, as specific\n",
    "characteristics of the data may lead you to explore different things in\n",
    "depth. However, this “recipe” can be a helpful starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Brooklyn Bridge pedestrian data set\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Brooklyn Bridge is a bridge that connects Brooklyn and Manhattan. It\n",
    "supports vehicles, pedestrians, and bikers.\n",
    "\n",
    "![](https://brooklyneagle.com/wp-content/uploads/2019/01/7-Brooklyn-Bridge-pedestrians-in-bike-lane-to-right-of-white-stripe-January-2019-photo-by-Lore-Croghan-600x397.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support you are developing a machine learning model to predict the\n",
    "volume of pedestrian traffic on the Brooklyn Bridge. There is a dataset\n",
    "available that you think may be useful as training data: [Brooklyn\n",
    "Bridge Automated Pedestrian Counts\n",
    "dataset](https://data.cityofnewyork.us/Transportation/Brooklyn-Bridge-Automated-Pedestrian-Counts-Demons/6fi9-q3ta),\n",
    "from the NYC Department of Transportation.\n",
    "\n",
    "We will practice applying the “recipe” for exploratory data analysis to\n",
    "this data.\n",
    "\n",
    "We will use the `pandas` library in Python, which includes many powerful\n",
    "utilities for managing data. You can refer to the [`pandas`\n",
    "reference](https://pandas.pydata.org/pandas-docs/stable/reference/index.html)\n",
    "for more details on the `pandas` functions used in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set down *expectations* about the data\n",
    "\n",
    "The first step is to codify your expectations about the data *before*\n",
    "you look at it:\n",
    "\n",
    "-   Read about *methodology* and *data codebook*\n",
    "-   How many rows and columns are in the data?\n",
    "-   What does each variable mean? What units are data recorded in? What\n",
    "    is the expected range or typical value for each column?\n",
    "-   What variables do you think could be used as target variable? What\n",
    "    variables could be used as features from which to learn?\n",
    "-   How was data collected? Identify sampling issues, timeliness issues,\n",
    "    fairness issues, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Brooklyn Bridge dataset, you can review the associated\n",
    "documentation on the NYC Data website:\n",
    "\n",
    "-   [NYC Data\n",
    "    Website](https://data.cityofnewyork.us/Transportation/Brooklyn-Bridge-Automated-Pedestrian-Counts-Demons/6fi9-q3ta)\n",
    "-   [Data\n",
    "    dictionary](https://data.cityofnewyork.us/api/views/6fi9-q3ta/files/845905ea-21d4-4ec7-958a-a1a09214513d?download=true&filename=Brooklyn_Bridge_Automated_Pedestrian_Counts_Demonstration_Project_data_dictionary.xlsx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and check that it is loaded correctly\n",
    "\n",
    "The next step is to load the data in preparation for our exploratory\n",
    "data analysis. Then, we’ll check that it is loaded correctly.\n",
    "\n",
    "Some examples of the things we’ll look for include:\n",
    "\n",
    "-   Does the `DataFrame` have the correct number of rows and columns\n",
    "    (consistent with our expectations from the first step)?\n",
    "-   Is the first row of “data” in the `DataFrame` real data, or is it\n",
    "    column labels that were misinterpreted as data? (Similarly, are the\n",
    "    column labels actually labels, or are they the first row of data?)\n",
    "-   Are the data types of every column consistent with our expectations?\n",
    "\n",
    "At this stage, we might also do some very basic manipulation of the data\n",
    "- for example, compute some fields that are derived directly from other\n",
    "fields. (For example, suppose you have a “distance” field in miles and\n",
    "you wanted to convert it to meters - you could do that here!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import some useful libraries:\n",
    "\n",
    "-   In Python - libraries add powerful functionality\n",
    "-   You can import an entire library (`import foo`) or part\n",
    "    (`from foo import bar`)\n",
    "-   You can define a nickname, which you will use to call functions of\n",
    "    these libraries (many libraries have “conventional” nicknames)\n",
    "\n",
    "`pandas` is a popular Python library for working with data. It is\n",
    "conventionally imported with the `pd` nickname."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# set up notebook to show all outputs, not only last one\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to read in our data!\n",
    "\n",
    "The main type of data structure in `pandas` is a `DataFrame`, which\n",
    "organizes data into a 2D table, like a spreadsheet. Unlike a `numpy`\n",
    "array, however, each column in a `DataFrame` can have different data\n",
    "types - for example, you can have a string column, an integer column,\n",
    "and a float column all in the same `DataFrame`.\n",
    "\n",
    "(The other major type of data in `pandas` is a `Series`, which is like a\n",
    "1D array- any individual row or column from a `DataFrame` will be a\n",
    "`Series`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You *can* create a `DataFrame` or a `Series` “by hand” - for example,\n",
    "try\n",
    "\n",
    "``` python\n",
    "pd.Series([1,2,3,99])\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "``` python\n",
    "pd.DataFrame({'fruit': ['apple', 'banana', 'kiwi'], 'cost': [0.55, 0.99, 1.24]})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But usually, we’ll read in data from a file.\n",
    "\n",
    "Our data for this Brooklyn Bridge example is in CSV format, so will use\n",
    "the `read_csv` function in `pandas` to read in our data. This function\n",
    "accepts a URL or a path to a file, and will return our data as a\n",
    "`DataFrame`.\n",
    "\n",
    "Function documentation: [pandas\n",
    "reference](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)\n",
    "\n",
    "``` python\n",
    "pandas.read_csv(filepath_or_buffer, \n",
    "    sep=',', header='infer', \n",
    "    names=None,\n",
    "    ...)\n",
    "```\n",
    "\n",
    "`read_csv` is for “flat” text files, where each data point is on another\n",
    "row, and the fields in a row are separated by some delimiter\n",
    "(e.g. comma). Other pandas functions exist for loading other kinds of\n",
    "data (read from database, Excel file, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://data.cityofnewyork.us/api/views/6fi9-q3ta/rows.csv?accessType=DOWNLOAD'\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will want to verify that the data was loaded correctly. For *tabular*\n",
    "data, we can start by looking at the first few rows of data or the last\n",
    "few rows of data with the `head` and `tail` functions, respectively.\n",
    "(For data that is not tabular, such as image, text, or audio data, we\n",
    "would similarly start by looking at some samples.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a few random rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at some rows can help us spot obvious problems with data\n",
    "loading. For example, suppose we had tried to read in the data using a\n",
    "tab delimiter to separate fields on the same row, instead of a comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bad  = pd.read_csv(url, sep='\\t')\n",
    "df_bad.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This “bad” version of the `DataFrame` has only a single column (because\n",
    "it believes tabs are used to separate fields in the same row, when\n",
    "actually commas are used). The variable names are combined together into\n",
    "one long column name. By looking at the first few rows of data, we can\n",
    "spot this obvious error.\n",
    "\n",
    "Here is another example of a “bad” `DataFrame`. Suppose we tell\n",
    "`read_csv` that the data file itself does not have a header row at the\n",
    "top, with column names in it; instead, we supply column names ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"hour_beginning\", \"location\", \"Pedestrians\", \"Towards Manhattan\", \n",
    "    \"Towards Brooklyn\", \"weather_summary\", \"temperature\", \"precipitation\", \n",
    "    \"lat\", \"long\", \"events\", \"Location1\"]\n",
    "df_bad  = pd.read_csv(url, header=None, names=col_names)\n",
    "df_bad.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the first row in the file *is* actually a column\n",
    "header, and we mistakenly read it in as data. (A similar problem can\n",
    "occur in reverse - if we told `read_csv` that the first row *is* a\n",
    "header when it is not, then our “column labels” would actually be the\n",
    "first row of data.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should always check the shape of the data frame - the number of rows\n",
    "and columns. This, too, should be checked against our assumptions about\n",
    "the data (in this case, what we know from the NYC Data website.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the names of the columns and their data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main data types we’ll see most often are `int64` (integer),\n",
    "`float64` (numeric), `bool` (True or False), or `object` (which includes\n",
    "string)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a quick summary with `info()`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` infers the data type of each column automatically from the\n",
    "contents of the data.\n",
    "\n",
    "If the data type of a column is not what you expect it to be, this can\n",
    "often be a signal that the data needs cleaning. For example, if you\n",
    "expect a column to be numeric and it is read in as non-numeric, this\n",
    "indicates that there are probably some samples that include a\n",
    "non-numeric value in that column. (The [NYC Data\n",
    "website](https://data.cityofnewyork.us/Transportation/Brooklyn-Bridge-Automated-Pedestrian-Counts-Demons/6fi9-q3ta)\n",
    "indicates what type of data *should* be in each column, so you should\n",
    "reference that when checking this output. )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a date/time column (`hour_beginning`) that was read in as a\n",
    "string. Let’s take a closer look at that. We can get one column of data\n",
    "either using a notation like a dictionary, as in\n",
    "\n",
    "``` python\n",
    "df['hour_beginning']\n",
    "```\n",
    "\n",
    "or using class attribute-like notation, as in\n",
    "\n",
    "``` python\n",
    "df.hour_beginning\n",
    "```\n",
    "\n",
    "(either one returns exactly the same thing!) (Note that if the column\n",
    "name includes spaces, you can only use the notation with the brackets,\n",
    "since it encloses the column name in quotes.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` includes a `to_datetime` function to convert this string to a\n",
    "“native” date/time format, so we can use that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour_beginning'] = pd.to_datetime(df['hour_beginning'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that the `hour_beginning` variable includes the full date\n",
    "and time in one field. For our analysis, it would be more useful to have\n",
    "separate fields for the date, month, day of the week, and hour.\n",
    "\n",
    "We can create these additional fields by assigning the desired value to\n",
    "them directly - then, observe the effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour'] = df['hour_beginning'].dt.hour\n",
    "df['month'] = df['hour_beginning'].dt.month\n",
    "df['date'] = df['hour_beginning'].dt.date\n",
    "df['day_name'] = df['hour_beginning'].dt.day_name()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect (and possibly clean/filter) the data\n",
    "\n",
    "Now we are ready to inspect the data.\n",
    "\n",
    "Some examples of the things we’ll look for include:\n",
    "\n",
    "-   Are there missing values? There may be rows *in* the data where some\n",
    "    or all fields are missing (which can be denoted as None, NaN, or\n",
    "    even 0 or -1 - which can be misleading when 0 or -1 are also valid\n",
    "    values for that field.) There may also be rows *not in* the data,\n",
    "    that we expect *should be* in the data.\n",
    "-   For numeric fields: Is the min and max of each field consistent with\n",
    "    our expectation? Is the median consistent with our expectation?\n",
    "-   For non-numeric fields: Are the number of unique values in each\n",
    "    field consistent with our expectations? Are the values of the factor\n",
    "    levels (where these can reasonably be assessed) described\n",
    "    consistently throughout the data?\n",
    "-   Are the relationships *between* variables consistent with our\n",
    "    expectations? (We can evaluate this visually, and also by looking at\n",
    "    summary statistics.)\n",
    "-   If the data is a time series, is the trend of each variable over\n",
    "    time consistent with our expectations?\n",
    "\n",
    "For many of these “sanity checks”, we will need some *domain knowledge*.\n",
    "It’s hard to have reasonable expectations about the values in the data\n",
    "if you do not understand the topic that the data relates to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check whether data is complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by checking whether the data is complete. First, we’ll\n",
    "check whether there are any rows in the data where some or all fields\n",
    "are missing.\n",
    "\n",
    "We can see the number of missing values in each column by summing up all\n",
    "the instances where the `isnull` function returns a True value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that this only tells us about missing values that are explicitly\n",
    "denoted as such - for example, explicit `NaN` values. If a missing value\n",
    "is coded as something else - like a 0 or -1 - we wouldn’t know unless we\n",
    "noticed an unusually high frequency of 0 or -1 values.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the majority of rows are missing a value in the `events`\n",
    "field, which is used to mark dates that are holidays or other special\n",
    "events. This is reasonable, since most dates do not have any remarkable\n",
    "events.\n",
    "\n",
    "Let’s look at the rows that *do* have a value in the `events` field. To\n",
    "filter a dataframe, we’ll use the `.loc[]` operator. This accepts either\n",
    "an index (for example, we can do `df.loc[0]` to see the first record in\n",
    "the dataframe), an array of indices (for example, `df.loc[[0,1,2]]`), or\n",
    "an array of boolean values the length of the entire dataframe. That’s\n",
    "what we’ll use here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['events'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also notice a small number of rows missing weather information. It’s\n",
    "not clear why these are missing. Let’s take a closer look at some of\n",
    "those rows, by *filtering* the dataframe to only rows that meet a\n",
    "specific condition - in this case, that the `temperature` field is\n",
    "missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.temperature.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for these particular instances, all of the weather\n",
    "information is missing. There’s no obvious reason or pattern. We’ll deal\n",
    "with these soon, when we try to clean/filter the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do that, though, let’s check for the *other* kind of missing\n",
    "data: rows that are missing completely, that we expect *should* be\n",
    "present.\n",
    "\n",
    "In this example, the data is a time series, and we expect that there is\n",
    "exactly one row of data for every single hour over the time period in\n",
    "which this data was collected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see if the data is complete, or if there are gaps in time.\n",
    "\n",
    "First, we will use\n",
    "[`pd.date_range`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.date_range.html)\n",
    "to get the list of hour intervals that we expect to find in the dataset.\n",
    "Then, we will find the difference between this list and the actual list\n",
    "of hour intervals in the dataset - these are missing intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get beginning and end of date range\n",
    "min_dt = df.hour_beginning.min()\n",
    "max_dt = df.hour_beginning.max()\n",
    "print(min_dt)\n",
    "print(max_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_range = pd.date_range(start = min_dt, end = max_dt, freq='H' )\n",
    "expected_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then identify the missing hours\n",
    "missing_hours = expected_range.difference(df['hour_beginning'])\n",
    "print(missing_hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had the expected number of rows (the output of `shape` matched the\n",
    "description of the data on the NYC Data website), but the data seems to\n",
    "be missing samples from August 2018 through December 2018, which is\n",
    "worth keeping in mind if we decide to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(missing_hours.date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s also check if any hour appears more than once in the data. We can\n",
    "use `pandas`’s `value_counts` function to find out how many times each\n",
    "unique value appears in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour_beginning'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like at least one hour appears twice in the data, which is\n",
    "unexpected! Let’s use filtering again to find out all of the instances\n",
    "where that occurs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_counts = df['hour_beginning'].value_counts()\n",
    "hour_counts.loc[hour_counts > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to happen exactly once. Let’s filter the dataframe to find the\n",
    "rows corresponding to the duplicate day.\n",
    "\n",
    "Here’s a useful clue - we can see that this hour appears twice because\n",
    "the clock is shifted for Daylight Savings time. (It’s not clear why\n",
    "there is no duplicate hour for that same event in 2017. Perhaps only one\n",
    "of those hours is recorded.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['hour_beginning']==\"2019-11-03 01:00:00\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have evaluated the “completeness” of our data, we have to\n",
    "decide what to do about missing values.\n",
    "\n",
    "Some machine learning models cannot tolerate data with missing values.\n",
    "Depending on what *type* of data is missing and *why* it is missing, we\n",
    "can\n",
    "\n",
    "-   drop rows with missing values from the dataset\n",
    "-   fill in (“impute”) the missing values with some value: a 0, the mode\n",
    "    of that column, the median of that column, or forward/back fill data\n",
    "    from the nearest row that is not missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this data, let’s try the forward/back fill method. This makes some\n",
    "sense because the data has a logical order in time, and the missing\n",
    "value - weather - changes relatively slowly with respect to time. We can\n",
    "expect that the weather at any given hour is probably similar to the\n",
    "weather in the previous (or next) hour.\n",
    "\n",
    "For this to work, we’ll first have to sort the data by time. (Note that\n",
    "the data was not sorted originally.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by='hour_beginning')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also “reset” the index now, so that if we ask for `df.loc[0]`\n",
    "we’ll get the first row in time, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fill in missing data using the `fillna` function\n",
    "([reference](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html)).\n",
    "We will fill the missing weather data using the “forward fill” method,\n",
    "which caries the last valid observation forward to fill in NAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['temperature'] = df['temperature'].fillna(method=\"ffill\")\n",
    "df['precipitation'] = df['precipitation'].fillna(method=\"ffill\")\n",
    "df['weather_summary'] = df['weather_summary'].fillna(method=\"ffill\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having imputed missing vaules in the weather-related columns, we can\n",
    "count the NAs again and find that there are only missing values in the\n",
    "`events` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validating expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some idea of the completeness of the data, let’s look\n",
    "at whether the data values are consistent with our expectations.\n",
    "\n",
    "To start, let’s look at summary statistics. The “five number summary” -\n",
    "extremes (min and max), median, and quartiles -can help us gain a better\n",
    "understanding of numeric fields in the data, and see whether they have\n",
    "reasonable values. We can use the `describe` function in `pandas` to\n",
    "compute this summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can only compute those summary statistics for numerical variables.\n",
    "For categorical variables, we can use `value_counts()` to get frequency\n",
    "of each value.\n",
    "\n",
    "For example, let’s see how often each `weather` condition occurs, and\n",
    "whether it is reasonable for NYC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.weather_summary.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s also useful to verify expected relationships.\n",
    "\n",
    "For example, we expect to see precipitation when the weather is rainy.\n",
    "We can use `groupby` in `pandas` to capture the effect between a\n",
    "categorical variable (`weather_summary`) and a numerical one,\n",
    "`precipitation`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('weather_summary')['precipitation'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make special note of the `count` column, which shows us the prevalence\n",
    "of different weather conditions in this dataset. There are some weather\n",
    "conditions for which we have very few examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can validate our expectation of hotter weather in the\n",
    "summer months:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('month')['temperature'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as well as during the middle of the day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('hour')['temperature'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a pairplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tabular data with multiple numeric features, it is often useful to\n",
    "create a *pairplot*. A pairplot shows pairwise relationships between all\n",
    "numerical variables. It is a useful way to identify variables that have\n",
    "a relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a “default” pairplot with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, each pane shows one numerical variable on the x-axis and another\n",
    "numerical variable on the y-axis, so that we can see if a relationship\n",
    "exists between them. The panes along the diagonal shows the empirical\n",
    "distribution of values for each feature in this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, it is difficult to see anything useful because there is so much\n",
    "going on in this plot. We can improve things somewhat by:\n",
    "\n",
    "-   specifying only the variables we want to include, and exluding\n",
    "    variables that don’t contain useful information, such as `lat` and\n",
    "    `long`, and\n",
    "-   making the points on the plot smaller and partially transparent, to\n",
    "    help with the overplotting.\n",
    "\n",
    "We’ll also change the histograms on the diagonal, which show the\n",
    "frequency of values for each variable, into a density plot which shows\n",
    "the same information in a more useful format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, \n",
    "             vars=['Pedestrians', 'temperature', 'precipitation', 'hour', 'month'],\n",
    "             diag_kind = 'kde',\n",
    "             plot_kws={'alpha':0.5, 'size': 0.1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot validates the relationship between `temperature` and `hour`,\n",
    "and between `temperature` and `month`. However, we can also use this\n",
    "plot to identify useful features - features that appear to be related to\n",
    "the `target` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore relationships and identify target variable and features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, since our goal is to train a machine learning model, we want to\n",
    "identify:\n",
    "\n",
    "-   an appropriate target variable - something on which to train our\n",
    "    model. (Either a direct target variable, or a proxy.)\n",
    "-   features that are predictive - if there is any noticeable\n",
    "    relationship between the target variable and any other variable,\n",
    "    this is likely to be a useful feature.\n",
    "-   features that are correlated with one another - if two features are\n",
    "    highly correlated, this presents some difficulty to certain types of\n",
    "    models, so we’ll want to know about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Pedestrians` variable is the obvious target variable for this\n",
    "learning problem: it’s exactly the quantity we want to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify potential predictive features among the numeric variables in\n",
    "the data, we can use the pairplot. Look at the row of the pairplot in\n",
    "which `Pedestrians` is on the vertical axis, and each of the other\n",
    "variables in turn is on the horizontal axis. Which of these seem to show\n",
    "a relationship? (Note: the relationship does not necessarily need to be\n",
    "a linear relationship.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also want to evaluate the categorical variables. For example, to\n",
    "look for a relationship between day of the week and pedestrian volume,\n",
    "we can group by `day_name`, then call the `describe` function on the\n",
    "`Pedestrians` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('day_name')['Pedestrians'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can see the effect of weather:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('weather_summary')['Pedestrians'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the effect of various holidays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('events')['Pedestrians'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now armed with information about these relationships, we can identify\n",
    "good candidate features for a machine learning model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('itp2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "152200e2ccd6411d8497b614a97aafc99f21ff398a9edf3fadef52bdd57caf02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
